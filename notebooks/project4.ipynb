{
  "cells": [
    {
      "cell_type": "markdown",
      "source": "# Project 4: Natural Language Processing \n---\n\n**Group 9: Aidan Stocks, Hugo Reinicke, Nicola Clark, Jonas-Mika Senghaas**\n\nSubmission: *03.06.2021* / Last Modified: *27.04.2021*\n\n---\n\nThis notebook contains the step-by-step data science process performed *XXX*. The goal of this project was to *XXX*.\n\nThe initial data was obtained from the [TweetEval](https://github.com/cardiffnlp/tweeteval#evaluating-your-system) GitHub repository, that provides data for supervised training of classifiers for natural language processing.",
      "metadata": {
        "cell_id": "00000-8f8eb98f-2a98-41af-9825-5a8a8c441dee",
        "tags": [],
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": "## Introduction\n---\n*XXX*",
      "metadata": {
        "cell_id": "00001-5fcfa044-05ac-485a-8293-0db98813ff5e",
        "tags": [],
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": "## Running this Notebook\n---\nThis notebook contains all code to reproduce the findings of the project as can be seen on the [GitHub](https://github.com/jonas-mika/fyp2021p04g09) page of this project. In order to read in the data correctly, the global paths configured in the section `Constants` need to be correct. The following file structure - as prepared in the `submission.zip` - was followed throughout the project and is recommended to use (alternatively the paths in the section `Constants` can be adjusted):\n\n```\n*project tree structure*\n```\n*Note that the rest of the file structure as can be seen on the [GitHub](https://github.com/jonas-mika/fyp2021p03g09) page of the project generates automatically*",
      "metadata": {
        "cell_id": "00002-e3f8c8e5-1dc7-47ff-9108-2450997c051c",
        "tags": [],
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": "## Required Libraries and Further Imports\n---\nThroughout the project, we will use a range of both built-in and external Python libraries. This notebook will only run if all libraries and modules are correctly installed on your local machines. \nTo install missing packages use `pip install <package_name>` (PIP (Python Package Index) is the central package management system, read more [here](https://pypi.org/project/pip/)). \n\nIn case you desire further information about the used packages, click the following links to find detailed documentations:\n- [Pandas](https://pandas.pydata.org/)\n- [Numpy](https://numpy.org/)\n- [Matplotlib](https://matplotlib.org/stable/index.html)\n- [PIL](https://pillow.readthedocs.io/en/stable/)\n- [SciKit Learn](https://scikit-learn.org/stable/)\n- [SciKit Image](https://scikit-image.org/)\n- [Scipy](https://www.scipy.org/)",
      "metadata": {
        "cell_id": "00003-8f195f8a-4025-446a-8b44-300b55666211",
        "tags": [],
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "00004-cf13b571-fc4d-40f4-8617-43224164fcb8",
        "deepnote_to_be_reexecuted": false,
        "execution_millis": 3149,
        "execution_start": 1620140213606,
        "source_hash": "56190a1e",
        "tags": [],
        "output_cleared": true,
        "deepnote_cell_type": "code"
      },
      "source": "%%capture\n# uncomment lines with uninstalled packages\n\n#!pip install -U numpy pandas matplotlib seaborn skikit-learn \n!pip install pycontractions",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "00005-9cbe0305-7f30-486b-be81-0c10af861246",
        "deepnote_to_be_reexecuted": false,
        "execution_millis": 0,
        "execution_start": 1620144638541,
        "source_hash": "474802c1",
        "tags": [],
        "output_cleared": true,
        "deepnote_cell_type": "code"
      },
      "source": "# python standard libraries\nimport json                                            # data transfer to and from json format\nimport os                                              # access operating system from python\nimport math                                            # mathematical operations in python\nimport random                                          # creates randomness\nimport re                                              # regex search in python\nimport shutil                                          # system control in python\nimport warnings                                        # ignore annoying warnings\nwarnings.filterwarnings(\"ignore\")\n\n# external libraries\nimport numpy as np                                     # used for numerical calculations and fast array manipulations\nimport pandas as pd                                    # provides major datastructure pd.DataFrame() to store datasets\nimport matplotlib\nimport matplotlib.pyplot as plt                        # basic data visualisation\nimport seaborn as sns                                  # advanced data visualisation\nfrom nltk.tokenize import TweetTokenizer               # tokeniser api\nfrom pycontractions import Contractions                # intelligently expands contractions in natural language\nfrom collections import Counter                        # counts objects",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "00006-5afa390e-a19c-4f7b-b2f5-752fba00777d",
        "deepnote_to_be_reexecuted": false,
        "execution_millis": 6,
        "execution_start": 1620140220795,
        "source_hash": "9a25b5af",
        "output_cleared": true,
        "deepnote_cell_type": "code"
      },
      "source": "print(f'Numpy Version: {np.__version__}')\nprint(f'Pandas Version: {pd.__version__}')\nprint(f'Matplotlib Version: {matplotlib.__version__}')\nprint(f'Seaborn Version: {sns.__version__}')",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "Since this project makes heavy use of functions to achieve maximal efficiency, all functions are stored externally in the package structure `project3'. The following imports are necessary for this notebook to run properly.",
      "metadata": {
        "cell_id": "00006-e9706598-54b2-4f29-9ca9-a6660685b48c",
        "tags": [],
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "00007-14d719d0-f91a-4eac-adc0-8f3981566bf1",
        "deepnote_to_be_reexecuted": false,
        "execution_millis": 2,
        "execution_start": 1619683192963,
        "source_hash": "905c667e",
        "tags": [],
        "output_cleared": true,
        "deepnote_cell_type": "code"
      },
      "source": "#from project4.processing import ...\n#from project4.save import ...\n#from project4.features import ...",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "## Constants\n---\nTo enhance readibilty, as well as to decrease the maintenance effort, it is useful for bigger projects to define contants that need to be accessed globally throughout the whole notebook in advance. \nThe following cell contains all of those global constants. By convention, we write them in caps (https://www.python.org/dev/peps/pep-0008/#constants)",
      "metadata": {
        "cell_id": "00008-4f347db9-f433-4a57-9e83-4fbb27586617",
        "tags": [],
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "code",
      "source": "PREPROCESS_DATA = False",
      "metadata": {
        "tags": [],
        "cell_id": "00010-18532f0a-c77d-42e2-b487-aa327b69dc04",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "2f89d05f",
        "execution_start": 1620140223266,
        "execution_millis": 1,
        "output_cleared": true,
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "00009-8c033e56-02ab-48b7-9eff-1efefe08d35c",
        "deepnote_to_be_reexecuted": false,
        "execution_millis": 1,
        "execution_start": 1620140366284,
        "source_hash": "599a5394",
        "tags": [],
        "output_cleared": true,
        "deepnote_cell_type": "code"
      },
      "source": "DATASETS = ['hate', 'emotion']\n\n# store paths\nPATH = {}\nPATH['data'] = {}\nPATH['data']['raw'] = \"../data/raw/\"\nPATH['data']['processed'] = \"../data/processed/\"\n\n# store data \nDATA = {}\nDATA['raw'] = {}\nDATA['processed'] = {}\nfor dataset in DATASETS:\n    DATA['raw'][dataset] = {}\n    DATA['processed'][dataset] = {}",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "*TASK 0*\n# Fetching Data\n---",
      "metadata": {
        "cell_id": "00010-30f29e03-2c93-4a41-9cdd-c63ddcf81385",
        "tags": [],
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": "## Loading in Data\n---",
      "metadata": {
        "cell_id": "00012-517a92e6-641e-4b58-ae16-4fc3f1dc0f0c",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "00011-d60e995b-fcb6-4868-b5f3-bead2a4d235a",
        "deepnote_to_be_reexecuted": false,
        "execution_millis": 0,
        "execution_start": 1620140546643,
        "source_hash": "74974d65",
        "tags": [],
        "output_cleared": true,
        "deepnote_cell_type": "code"
      },
      "source": "def read_raw_input(dataset):\n    # reading in all .txts into list of strings\n    for _file in os.listdir(f'../data/raw/{dataset}'):\n        with open(f'../data/raw/{dataset}/{_file}', 'r', encoding='UTF-8') as infile:\n            DATA['raw'][dataset][_file[:-4]] = [line.strip() for line in infile.readlines()]\n\n    # convert target labels to integers\n    for key in ['train_labels', 'val_labels', 'test_labels']:\n        DATA['raw'][dataset][key] = [int(x) for x in DATA['raw'][dataset][key]]\n\n    # convert mapping to dictionary\n    DATA['raw'][dataset]['mapping'] = {int(string.split('\\t')[0]): string.split('\\t')[1] for string in DATA['raw'][dataset]['mapping']}",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "00014-a564b5fd-9112-40ea-ab20-a9e06cef672c",
        "deepnote_to_be_reexecuted": false,
        "execution_millis": 50,
        "execution_start": 1620140546904,
        "scrolled": false,
        "source_hash": "3fbf30af",
        "output_cleared": true,
        "deepnote_cell_type": "code"
      },
      "source": "for dataset in DATASETS:\n    read_raw_input(dataset)",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "## Exploring Data\n---",
      "metadata": {
        "cell_id": "00015-6ac2c66d-4d76-4711-82ed-29c0f25e3314",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": "### Peek into Training Tweets",
      "metadata": {
        "cell_id": "00016-1ade1bb5-34a5-4a97-9049-49dbb00ee154",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "00017-8c456ad3-3f69-4ec0-931f-2fdc6fe0e33e",
        "deepnote_to_be_reexecuted": false,
        "execution_millis": 5,
        "execution_start": 1620140575455,
        "source_hash": "4847394d",
        "output_cleared": true,
        "deepnote_cell_type": "code"
      },
      "source": "# hate\nfor i in range(10):\n    print(f\"{i+1}\\tLabel: {DATA['raw']['hate']['mapping'][DATA['raw']['hate']['train_labels'][i]].title()}\\t\\t{DATA['raw']['hate']['train_text'][i]}\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "00018-f6587a74-f6da-4c6c-8540-db447f5615d9",
        "deepnote_to_be_reexecuted": false,
        "execution_millis": 5,
        "execution_start": 1620140591082,
        "source_hash": "c303da21",
        "output_cleared": true,
        "deepnote_cell_type": "code"
      },
      "source": "# emotion\nfor i in range(10):\n    print(f\"{i+1}\\tLabel: {DATA['raw']['emotion']['mapping'][DATA['raw']['emotion']['train_labels'][i]].title()}\\t\\t{DATA['raw']['emotion']['train_text'][i]}\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "### Visualising Label Distribution",
      "metadata": {
        "cell_id": "00019-ac0093ee-cb8e-4df0-9240-28b8639da8f0",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "00020-ff2b603d-7d95-43b5-8411-7c54ff2985ec",
        "deepnote_to_be_reexecuted": false,
        "execution_millis": 1,
        "execution_start": 1620140612779,
        "source_hash": "f856703d",
        "output_cleared": true,
        "deepnote_cell_type": "code"
      },
      "source": "def visualise_label_distribution(dataset):\n    fig, ax = plt.subplots(ncols=3, figsize=(12,4))\n    fig.suptitle(f'Frequency of Target Label in {dataset.capitalize()}', fontsize=12, fontweight='bold')\n\n    for i, key in enumerate(['train_labels', 'val_labels', 'test_labels']):\n        label, count = np.unique(DATA['raw'][dataset][key], return_counts=True)\n        ax[i].bar(label, count, color='grey');\n        ax[i].set_title(key.replace('_', ' ').title())\n        ax[i].set_xticks(label); ax[i].set_xticklabels([string.title() for string in DATA['raw'][dataset]['mapping'].values()])",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "00021-046d0488-77dc-47d3-a29c-d178ea704a1f",
        "deepnote_to_be_reexecuted": false,
        "execution_millis": 283,
        "execution_start": 1620140614124,
        "source_hash": "894be1c8",
        "output_cleared": true,
        "deepnote_cell_type": "code"
      },
      "source": "visualise_label_distribution(dataset='hate')",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "00022-83c0ec2d-8094-4123-8603-caad77edb2af",
        "deepnote_to_be_reexecuted": false,
        "execution_millis": 301,
        "execution_start": 1620140615529,
        "source_hash": "c0926180",
        "output_cleared": true,
        "deepnote_cell_type": "code"
      },
      "source": "visualise_label_distribution(dataset='emotion')",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "*TASK 0.5*\n# Processing of Language Data\n---\n",
      "metadata": {
        "cell_id": "00011-c1d3e256-83ab-4833-93bb-155c6dcb73ca",
        "tags": [],
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": "### Tokenize Tweets\n---\n",
      "metadata": {
        "cell_id": "00024-925deca3-bb20-43eb-bc18-98e47d7a5780",
        "tags": [],
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "00024-20c29398-21ac-493b-8d2a-2161b831ef91",
        "deepnote_to_be_reexecuted": false,
        "execution_millis": 1,
        "execution_start": 1620140633934,
        "source_hash": "a1f7603b",
        "output_cleared": true,
        "deepnote_cell_type": "code"
      },
      "source": "def tokeniser1(tweet):\n    return re.findall('\\w+', tweet)",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "00013-de37fee0-8080-49a2-8348-0cc7cd793435",
        "deepnote_to_be_reexecuted": false,
        "execution_millis": 1,
        "execution_start": 1620140634312,
        "source_hash": "fcce8e84",
        "tags": [],
        "output_cleared": true,
        "deepnote_cell_type": "code"
      },
      "source": "def tokeniser2(tweet):\n    return re.split(' ', tweet)",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "00026-12aa65c5-fb65-4563-9518-2593493305f8",
        "deepnote_to_be_reexecuted": false,
        "execution_millis": 1,
        "execution_start": 1620140634939,
        "source_hash": "addc91b",
        "output_cleared": true,
        "deepnote_cell_type": "code"
      },
      "source": "def tokeniser3(tweet):\n    tk = TweetTokenizer()\n    return tk.tokenize(tweet)",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "00027-ef6625a3-be50-47af-8b54-b6da6068f0c4",
        "deepnote_to_be_reexecuted": false,
        "execution_millis": 1,
        "execution_start": 1620140651736,
        "source_hash": "70f25dd3",
        "output_cleared": true,
        "deepnote_cell_type": "code"
      },
      "source": "def tokeniser4(line):\n    # Initialise lists\n    tokens = []\n    unmatchable = [] # should be emtpy if done nicely\n\n    # Compile patterns for speedup\n    token_pat = re.compile('\\w+')\n    skippable_pat = re.compile('\\s+')  # typically spaces\n\n    # As long as there's any material left...\n    while line:\n        print(line)\n        # Try finding a skippable token delimiter first.\n        skippable_match = re.search(skippable_pat, line)\n        print(skippable_match)\n        if skippable_match and skippable_match.start() == 0:\n            # If there is one at the beginning of the line, just skip it.\n            line = line[skippable_match.end():]\n        else:\n            # Else try finding a real token.\n            token_match = re.search(token_pat, line)\n            if token_match and token_match.start() == 0:\n                print(token_match)\n                # If there is one at the beginning of the line, tokenise it.\n                tokens.append(line[:token_match.end()])\n                line = line[token_match.end():]\n            else:\n                # Else there is unmatchable material here.\n                # It ends where a skippable or token match starts, or at the end of the line.\n                unmatchable_end = len(line)\n                if skippable_match:\n                    unmatchable_end = skippable_match.start()\n                if token_match:\n                    unmatchable_end = min(unmatchable_end, token_match.start())\n                # Add it to unmatchable and discard from line.\n                unmatchable.append(line[:unmatchable_end])\n                line = line[unmatchable_end:]\n\n    return tokens",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "00028-15e617bc-1809-4253-8f95-c827822c2e9e",
        "deepnote_to_be_reexecuted": false,
        "execution_millis": 24,
        "execution_start": 1620140653158,
        "source_hash": "47651239",
        "output_cleared": true,
        "deepnote_cell_type": "code"
      },
      "source": "#ex_tweet = DATA['hate']['train_text'][0]\nex_tweet = \"https://t.co/9z2J3P33Uc FB needs to hurry up and add a laugh/cry button üò¨üò≠üòìü§¢üôÑüò± Since eating my feelings has not fixed the world's problems, I guess I'll try to sleep... HOLY CRAP: DeVos questionnaire appears to include passages from uncited sources https://t.co/FNRoOlfw9s well played, Senator Murray Keep the pressure on: https://t.co/4hfOsmdk0l @datageneral thx Mr Taussig It's interesting how many people contact me about applying for a PhD and don't @user spell my name #something right. Wow ok I didn't realise i'm using contractions like haven't and won't.\"\n\ntokeniser = [tokeniser1, tokeniser2, tokeniser3]\n\nprint('Original Tweet: ',ex_tweet, '\\n')\nfor i in range(len(tokeniser)):\n    print(f\"Tokeniser {i+1}: {tokeniser[i](ex_tweet)}\\n\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "### Summary\n---\n\nTokeniser 1 (Finds all Words):\n+ Good representation of words (includes Unicode characters and is sensible to punctuation within words, ie. \"don't\")\n- Deletes all punctuation (which might be important, ie. twitter communication heavily based tagging using '@' and hastags (#))\n- Destroys Links\n- Blind to Emojis\n\nTokeniser 2 (Split at Whitespace):\n+ Overall good representation of words \n+ keeps punctuation\n- ending punctuation is not being separated\n+ keeps Links\n+ recognises Emojis\n- doesnt separate adjacent emojis \n- blind to several words written together (in slang), ie. idontgiveafuck\n\nTokeniser 3 (Twitter Tokeniser API, Overcomes issues of Tokeniser 2):\n+ Ending punctuation is being separated\n+ Separate adjacent Emojis ",
      "metadata": {
        "cell_id": "00029-6b025e1e-41c8-40e8-95a6-ce98b58a469d",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": "### Improving Tokenizer",
      "metadata": {
        "cell_id": "00032-9918e5ff-ad34-4397-8a23-02f60d16a194",
        "tags": [],
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": "Improving on the Tweet Tokenizer:\n* lowercasing \n* expanding contractions\n* change links to a dummy value",
      "metadata": {
        "cell_id": "00032-4555a69b-e985-4c97-8a63-e922bdc51ee5",
        "tags": [],
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": "## Final Tokenising",
      "metadata": {
        "cell_id": "00042-494c027a-c333-45d2-95c0-f8775d64c2ee",
        "tags": [],
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "00043-2f55d04c-3946-4a00-a740-a8debdffb56c",
        "tags": [],
        "deepnote_to_be_reexecuted": false,
        "source_hash": "8e83866d",
        "execution_start": 1620140791986,
        "execution_millis": 1,
        "output_cleared": true,
        "deepnote_cell_type": "code"
      },
      "source": "# define url regex pattern and replace pattern globally (compile for speedup)\nurl_pattern = re.compile(r\"(?i)((?:https?://|www\\d{0,3}[.]|[a-z0-9.-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|(([^\\s()<>]+|(([^\\s()<>]+)))))+(?:(([^\\s()<>]+|(([^\\s()<>]+))))|[^\\s`!()[]{};:'\\\".,<>?¬´¬ª\\‚Äú\\‚Äù‚Äò‚Äô]))\")\ndummy = \" <=LINK=> \"\n\ndef remove_links(tweet):\n    return re.sub(url_pattern, dummy, tweet) ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "00043-6453b1f6-fe2d-4454-a660-25658b074aab",
        "deepnote_to_be_reexecuted": false,
        "execution_millis": 2,
        "execution_start": 1620140792809,
        "source_hash": "6b72decd",
        "tags": [],
        "output_cleared": true,
        "deepnote_cell_type": "code"
      },
      "source": "# loading pre-trained contraction model\ncont = Contractions(api_key=\"glove-twitter-100\")\n\ndef expand(tweet):\n    return list(cont.expand_texts([tweet]))[0]",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "00042-a9e30c04-e319-4b86-93a2-495d377ffdf7",
        "deepnote_to_be_reexecuted": false,
        "execution_millis": 1,
        "execution_start": 1620140793995,
        "source_hash": "57286f66",
        "tags": [],
        "output_cleared": true,
        "deepnote_cell_type": "code"
      },
      "source": "tk = TweetTokenizer()\n\ndef clean(tweet):\n    cleaned_tweet = expand(remove_links(tweet.lower()))\n    return tk.tokenize(cleaned_tweet)",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "## Testing clean function",
      "metadata": {
        "cell_id": "00042-19104ffb-e92b-4585-9351-30174aea6d80",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "00046-0a1e72d3-a195-4b4d-b338-b99415bcfd4a",
        "deepnote_to_be_reexecuted": false,
        "execution_millis": 13,
        "execution_start": 1619689533083,
        "scrolled": true,
        "source_hash": "53c4d941",
        "tags": [],
        "output_cleared": true,
        "deepnote_cell_type": "code"
      },
      "source": "clean(ex_tweet)",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "00046-8f2c5b62-9cce-4460-a430-78de3377ebca",
        "deepnote_to_be_reexecuted": false,
        "execution_millis": 1,
        "execution_start": 1620141529783,
        "source_hash": "6cf87b7c",
        "tags": [],
        "output_cleared": true,
        "deepnote_cell_type": "code"
      },
      "source": "if PREPROCESS_DATA == True:\n    for dataset in DATASETS:\n        # create preprocessed folder\n        try: os.makedirs(f'../data/processed/{dataset}')\n        except: None\n\n        # preprocess and tokenize tweets\n        for key in ['train_text', 'val_text', 'test_text']:\n            with open(f'../data/processed/{dataset}/{key}.txt', 'w', encoding = 'UTF-8') as outfile:\n                for tweet in DATA[dataset][key]:\n                    outfile.write('\\t'.join(clean(tweet)) + '\\n') # writing tokenised tweet with tab delimiter\n\n        # copy labels and mapping without preprocessing\n        for key in ['train_labels', 'val_labels', 'test_labels', 'mapping']:\n            shutil.copyfile(f'../data/raw/{dataset}/{key}.txt', f'../data/processed/{dataset}/{key}.txt')",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "00045-ccf8e1c1-82e9-4889-83a2-29cf5c12fb56",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "b7d638a4",
        "execution_start": 1620141052932,
        "execution_millis": 77,
        "output_cleared": true,
        "deepnote_cell_type": "code"
      },
      "source": "# loading in preprocessed data \nfor dataset in DATASETS: \n        for key in os.listdir(''):\n            with open(f'../data/processed/{dataset}/{key}.txt', 'r', encoding = 'UTF-8') as infile:\n                DATA['processed'][dataset][key]= [line.strip().split('\\t') for line in infile]",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# reading in processed data\nfor dataset in DATASETS:\n    for _file in os.listdir(f'../data/processed/{dataset}'):\n        with open(f'../data/processed/{dataset}/{_file}', 'r', encoding='UTF-8') as infile:\n            DATA['processed'][dataset][_file[:-4]] = [line.strip().split('\\t') for line in infile.readlines()]\n\n    # convert target labels to integers\n    for key in ['train_labels', 'val_labels', 'test_labels']:\n        DATA['processed'][dataset][key] = [int(x) for x in [DATA['processed'][dataset][key][i][0] for i in range(len(DATA['processed'][dataset][key]))]]\n\n    # convert mapping to dictionary\n    DATA['processed'][dataset]['mapping'] = {int(string[0]): string[1] for string in DATA['processed'][dataset]['mapping']}",
      "metadata": {
        "tags": [],
        "cell_id": "00042-632ac15c-7d49-4503-b780-5829b6f5faac",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "638e1903",
        "execution_start": 1620142391772,
        "execution_millis": 76,
        "output_cleared": true,
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "*TASK 1* \n# Descriptive Statistics about Natural Language Data\n---",
      "metadata": {
        "tags": [],
        "cell_id": "00044-cef6a6e4-c220-496f-ada8-d6cfbc08f299",
        "deepnote_cell_type": "markdown"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# build entire alphabet from training tweets\nVOCABULARY = {}\nfor dataset in DATASETS:\n    corpus = [token for tweet in DATA['processed'][dataset]['train_text'] for token in tweet]\n\n    # initialise frequency counter\n    VOCABULARY[dataset] = pd.DataFrame.from_dict(Counter(corpus), orient='index').rename(columns={'index': 'token', 0: 'frequency'}).sort_values(by='frequency', ascending=False).reset_index()\n\n    VOCABULARY[dataset]['rank'] = VOCABULARY[dataset].index + 1 # add one to index to build rank\n    VOCABULARY[dataset]['normalised_frequency'] = VOCABULARY[dataset]['frequency'] / VOCABULARY[dataset]['frequency'].sum() # relative frequency\n    VOCABULARY[dataset]['cumulative_frequency'] = VOCABULARY[dataset]['normalised_frequency'].cumsum()",
      "metadata": {
        "tags": [],
        "cell_id": "00042-f580c106-03ae-4ef2-a8de-3329cc7f39d7",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "12df028b",
        "execution_start": 1620145424800,
        "execution_millis": 25,
        "output_cleared": true,
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "VOCABULARY['hate']",
      "metadata": {
        "tags": [],
        "cell_id": "00046-5accac30-2f91-4cf6-8d65-ca5f6e2cfff1",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "76d6d744",
        "execution_start": 1620145431592,
        "execution_millis": 99,
        "output_cleared": true,
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "## Hate Plots",
      "metadata": {
        "tags": [],
        "cell_id": "00046-7c9fef28-200d-494f-b96d-c48da9560645",
        "deepnote_cell_type": "markdown"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "#dict(Counter(VOCABULARY['hate']['frequency']))\nimport math\nfrequency, frequency_of_frequency = np.unique(VOCABULARY['hate']['frequency'], return_counts=True)\nfig, ax = plt.subplots()\nax.scatter([math.log(f) for f in frequency], [math.log(f) for f in frequency_of_frequency],s=0.5)",
      "metadata": {
        "tags": [],
        "cell_id": "00047-55fe270c-7dbf-4329-91e6-1e349967f510",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "a043a749",
        "execution_start": 1620145428555,
        "execution_millis": 165,
        "output_cleared": true,
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# plots\nsns.set_theme(style='whitegrid')\n\n# Plot: Cumulative frequency by index\nsns.relplot(x='rank', y='cumulative_frequency', data=VOCABULARY['hate'], kind='line')\nsns.relplot(x='rank', y='cumulative_frequency', data=VOCABULARY['hate'][:2000], kind='line') # Cumulative frequency by index, top 10000 tokens\n\n# Plot: Log-log plot for Zipf's law\n#frq['log_frq'] = numpy.log(frq.frequency)\n#frq['log_rank'] = numpy.log(frq.frequency.rank(ascending=True))\n#seaborn.relp",
      "metadata": {
        "tags": [],
        "cell_id": "00045-a562d74f-2f2c-4791-9584-2f04196769f7",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "9b0afc9c",
        "execution_start": 1620145365899,
        "execution_millis": 1547,
        "output_cleared": true,
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# frequency plot (rank/ frequency)\nfig, ax = plt.subplots()\nax.scatter(VOCABULARY['hate']['rank'], VOCABULARY['hate']['frequency'],  s=2);\nax.set_title('Frequency Plot'); ax.set_xlabel('Rank'); ax.set_ylabel('Frequency');",
      "metadata": {
        "tags": [],
        "cell_id": "00051-d2c0bd6d-7e5a-4aa4-989f-07068434c1fc",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "c5d39fe3",
        "execution_start": 1620145445689,
        "execution_millis": 214,
        "output_cleared": true,
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# log-log plot for zipf's law\nfig, ax = plt.subplots()\nplt.scatter(np.log(VOCABULARY['hate']['rank']), np.log(VOCABULARY['hate']['frequency']), s=15,marker=\".\")\nax.set_title(\"Log-Log Frequency Plot (Visualising Zipf'Law)\"); ax.set_xlabel('Rank (Log)'); ax.set_ylabel('Frequency (Log)');",
      "metadata": {
        "tags": [],
        "cell_id": "00050-27ede284-d642-4c79-97d5-d69b0df6d717",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "35529072",
        "execution_start": 1620145483843,
        "execution_millis": 193,
        "output_cleared": true,
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "## Emotional Plots",
      "metadata": {
        "tags": [],
        "cell_id": "00048-0061151a-0e2e-4417-af54-813e14c015de",
        "deepnote_cell_type": "markdown"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "one_occurance=VOCABULARY['hate']['frequency']==1",
      "metadata": {
        "tags": [],
        "cell_id": "00045-4ab344ce-ccd0-464b-9e52-86f362864b29",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "77e6b1ff",
        "execution_start": 1620143071356,
        "execution_millis": 0,
        "output_cleared": true,
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "#VOCABULARY['hate'][one_occurance]",
      "metadata": {
        "tags": [],
        "cell_id": "00046-77715214-bd39-43d0-9f31-ecb127f58947",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "4f3c6e68",
        "execution_start": 1620143611535,
        "execution_millis": 0,
        "output_cleared": true,
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "*TASK 2*\n# XXX\n---",
      "metadata": {
        "cell_id": "00012-f7ed8c9e-4e5c-4499-8cbd-bc465deeb219",
        "tags": [],
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "00015-c0d67151-ed60-49d4-822d-0806e71831bc",
        "deepnote_to_be_reexecuted": false,
        "execution_millis": 3,
        "execution_start": 1619683193875,
        "source_hash": "5cd17fc",
        "tags": [],
        "output_cleared": true,
        "deepnote_cell_type": "code"
      },
      "source": "# some code",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "*TASK 3*\n# Open Question: XXX\n---",
      "metadata": {
        "cell_id": "00013-fc964c9b-2d90-48f7-93b6-89e1c191c669",
        "tags": [],
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "00017-2bcdfccd-1c4e-4b00-abcb-2807c0f7da9c",
        "deepnote_to_be_reexecuted": false,
        "execution_millis": 1,
        "execution_start": 1619683193881,
        "source_hash": "5cd17fc",
        "tags": [],
        "output_cleared": true,
        "deepnote_cell_type": "code"
      },
      "source": "# some code",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=7ea65742-831a-48cd-9d56-a22e1ed66c7b' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>",
      "metadata": {
        "tags": [],
        "created_in_deepnote_cell": true,
        "deepnote_cell_type": "markdown"
      }
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "deepnote": {
      "is_reactive": false
    },
    "deepnote_execution_queue": [],
    "deepnote_notebook_id": "912c8bda-48f1-43bf-8bff-0aac9dd9fbcb",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  }
}