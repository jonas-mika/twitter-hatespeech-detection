{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00000-8f8eb98f-2a98-41af-9825-5a8a8c441dee",
        "deepnote_cell_type": "markdown"
      },
      "source": [
        "# Project 4: XXX\n",
        "---\n",
        "\n",
        "**Group 9: Aidan Stocks, Hugo Reinicke, Nicola Clark, Jonas-Mika Senghaas**\n",
        "\n",
        "Submission: *03.06.2021* / Last Modified: *27.04.2021*\n",
        "\n",
        "---\n",
        "\n",
        "This notebook contains the step-by-step data science process performed *XXX*. The goal of this project was to *XXX*.\n",
        "\n",
        "The initial data *XXX*"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction\n",
        "---\n",
        "*XXX*"
      ],
      "metadata": {
        "tags": [],
        "cell_id": "00001-5fcfa044-05ac-485a-8293-0db98813ff5e",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Running this Notebook\n",
        "---\n",
        "This notebook contains all code to reproduce the findings of the project as can be seen on the [GitHub](https://github.com/jonas-mika/fyp2021p04g09) page of this project. In order to read in the data correctly, the global paths configured in the section `Constants` need to be correct. The following file structure - as prepared in the `submission.zip` - was followed throughout the project and is recommended to use (alternatively the paths in the section `Constants` can be adjusted):\n",
        "\n",
        "```\n",
        "*project tree structure*\n",
        "```\n",
        "*Note that the rest of the file structure as can be seen on the [GitHub](https://github.com/jonas-mika/fyp2021p03g09) page of the project generates automatically*"
      ],
      "metadata": {
        "tags": [],
        "cell_id": "00002-e3f8c8e5-1dc7-47ff-9108-2450997c051c",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Required Libraries and Further Imports\n",
        "---\n",
        "Throughout the project, we will use a range of both built-in and external Python libraries. This notebook will only run if all libraries and modules are correctly installed on your local machines. \n",
        "To install missing packages use `pip install <package_name>` (PIP (Python Package Index) is the central package management system, read more [here](https://pypi.org/project/pip/)). \n",
        "\n",
        "In case you desire further information about the used packages, click the following links to find detailed documentations:\n",
        "- [Pandas](https://pandas.pydata.org/)\n",
        "- [Numpy](https://numpy.org/)\n",
        "- [Matplotlib](https://matplotlib.org/stable/index.html)\n",
        "- [PIL](https://pillow.readthedocs.io/en/stable/)\n",
        "- [SciKit Learn](https://scikit-learn.org/stable/)\n",
        "- [SciKit Image](https://scikit-image.org/)\n",
        "- [Scipy](https://www.scipy.org/)"
      ],
      "metadata": {
        "tags": [],
        "cell_id": "00003-8f195f8a-4025-446a-8b44-300b55666211",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "# uncomment lines with uninstalled packages\n",
        "\n",
        "#!pip install -U numpy pandas matplotlib seaborn skikit-learn "
      ],
      "metadata": {
        "tags": [],
        "cell_id": "00004-cf13b571-fc4d-40f4-8617-43224164fcb8",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# python standard libraries\n",
        "import json                                            # data transfer to and from json format\n",
        "import os                                              # access operating system from python\n",
        "import random                                          # creates randomness\n",
        "import re                                              # regex search in python\n",
        "\n",
        "# external libraries\n",
        "import numpy as np                                     # used for numerical calculations and fast array manipulations\n",
        "import pandas as pd                                    # provides major datastructure pd.DataFrame() to store datasets\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt                        # basic data visualisation\n",
        "import seaborn as sns                                  # advanced data visualisation\n",
        "from nltk.tokenize import TweetTokenizer               # tokeniser api"
      ],
      "metadata": {
        "tags": [],
        "cell_id": "00005-9cbe0305-7f30-486b-be81-0c10af861246",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f'Numpy Version: {np.__version__}')\n",
        "print(f'Pandas Version: {pd.__version__}')\n",
        "print(f'Matplotlib Version: {matplotlib.__version__}')\n",
        "print(f'Seaborn Version: {sns.__version__}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since this project makes heavy use of functions to achieve maximal efficiency, all functions are stored externally in the package structure `project3'. The following imports are necessary for this notebook to run properly."
      ],
      "metadata": {
        "tags": [],
        "cell_id": "00006-e9706598-54b2-4f29-9ca9-a6660685b48c",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#from project4.processing import ...\n",
        "#from project4.save import ...\n",
        "#from project4.features import ..."
      ],
      "metadata": {
        "tags": [],
        "cell_id": "00007-14d719d0-f91a-4eac-adc0-8f3981566bf1",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Constants\n",
        "---\n",
        "To enhance readibilty, as well as to decrease the maintenance effort, it is useful for bigger projects to define contants that need to be accessed globally throughout the whole notebook in advance. \n",
        "The following cell contains all of those global constants. By convention, we write them in caps (https://www.python.org/dev/peps/pep-0008/#constants)"
      ],
      "metadata": {
        "tags": [],
        "cell_id": "00008-4f347db9-f433-4a57-9e83-4fbb27586617",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DATASETS = ['hate', 'emotion']\n",
        "\n",
        "# store paths\n",
        "PATH = {}\n",
        "PATH['data'] = {}\n",
        "PATH['data']['raw'] = \"../data/raw/\"\n",
        "PATH['data']['processed'] = \"../data/processed/\"\n",
        "\n",
        "# store data \n",
        "DATA = {}\n",
        "for dataset in DATASETS:\n",
        "    DATA[dataset] = {}"
      ],
      "metadata": {
        "tags": [],
        "cell_id": "00009-8c033e56-02ab-48b7-9eff-1efefe08d35c",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "*TASK 0*\n",
        "# Fetching Data\n",
        "---"
      ],
      "metadata": {
        "tags": [],
        "cell_id": "00010-30f29e03-2c93-4a41-9cdd-c63ddcf81385",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "source": [
        "## Loading in Data\n",
        "---"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "def read_raw_input(dataset):\n",
        "    # reading in all .txts into list of strings\n",
        "    for _file in os.listdir(f'../data/raw/{dataset}'):\n",
        "        with open(f'../data/raw/{dataset}/{_file}', 'r') as infile:\n",
        "            DATA[dataset][_file[:-4]] = [line.strip() for line in infile.readlines()]\n",
        "\n",
        "    # convert target labels to integers\n",
        "    for key in ['train_labels', 'val_labels', 'test_labels']:\n",
        "        DATA[dataset][key] = [int(x) for x in DATA[dataset][key]]\n",
        "\n",
        "    # convert mapping to dictionary\n",
        "    DATA[dataset]['mapping'] = {int(string.split('\\t')[0]): string.split('\\t')[1] for string in DATA[dataset]['mapping']}"
      ],
      "metadata": {
        "tags": [],
        "cell_id": "00011-d60e995b-fcb6-4868-b5f3-bead2a4d235a",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for dataset in DATASETS:\n",
        "    read_raw_input(dataset)"
      ]
    },
    {
      "source": [
        "## Exploring Data\n",
        "---"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "source": [
        "### Peek into Training Tweets"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# hate\n",
        "for i in range(10):\n",
        "    print(f\"{i+1}\\tLabel: {DATA['hate']['mapping'][DATA['hate']['train_labels'][i]].title()}\\t\\t{DATA['hate']['train_text'][i]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# hate\n",
        "for i in range(10):\n",
        "    print(f\"{i+1}\\tLabel: {DATA['emotion']['mapping'][DATA['emotion']['train_labels'][i]].title()}\\t\\t{DATA['emotion']['train_text'][i]}\")"
      ]
    },
    {
      "source": [
        "### Visualising Label Distribution"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def visualise_label_distribution(dataset):\n",
        "    fig, ax = plt.subplots(ncols=3, figsize=(12,4))\n",
        "    fig.suptitle(f'Frequency of Target Label in {dataset.capitalize()}', fontsize=12, fontweight='bold')\n",
        "\n",
        "    for i, key in enumerate(['train_labels', 'val_labels', 'test_labels']):\n",
        "        label, count = np.unique(DATA[dataset][key], return_counts=True)\n",
        "        ax[i].bar(label, count, color='grey');\n",
        "        ax[i].set_title(key.replace('_', ' ').title())\n",
        "        ax[i].set_xticks(label); ax[i].set_xticklabels([string.title() for string in DATA[dataset]['mapping'].values()])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "visualise_label_distribution(dataset='hate')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "visualise_label_distribution(dataset='emotion')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*TASK 0.5*\n",
        "# Processing of Language Data\n",
        "---\n"
      ],
      "metadata": {
        "tags": [],
        "cell_id": "00011-c1d3e256-83ab-4833-93bb-155c6dcb73ca",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def tokeniser1(tweet):\n",
        "    return re.findall('\\w+', tweet)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tokeniser2(tweet):\n",
        "    return re.split(' ', tweet)"
      ],
      "metadata": {
        "tags": [],
        "cell_id": "00013-de37fee0-8080-49a2-8348-0cc7cd793435",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def tokeniser3(tweet):\n",
        "    tk = TweetTokenizer()\n",
        "    return tk.tokenize(tweet)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def tokeniser4(line):\n",
        "    # Initialise lists\n",
        "    tokens = []\n",
        "    unmatchable = [] # should be emtpy if done nicely\n",
        "\n",
        "    # Compile patterns for speedup\n",
        "    token_pat = re.compile('\\w+')\n",
        "    skippable_pat = re.compile('\\s+')  # typically spaces\n",
        "\n",
        "    # As long as there's any material left...\n",
        "    while line:\n",
        "        print(line)\n",
        "        # Try finding a skippable token delimiter first.\n",
        "        skippable_match = re.search(skippable_pat, line)\n",
        "        print(skippable_match)\n",
        "        if skippable_match and skippable_match.start() == 0:\n",
        "            # If there is one at the beginning of the line, just skip it.\n",
        "            line = line[skippable_match.end():]\n",
        "        else:\n",
        "            # Else try finding a real token.\n",
        "            token_match = re.search(token_pat, line)\n",
        "            if token_match and token_match.start() == 0:\n",
        "                print(token_match)\n",
        "                # If there is one at the beginning of the line, tokenise it.\n",
        "                tokens.append(line[:token_match.end()])\n",
        "                line = line[token_match.end():]\n",
        "            else:\n",
        "                # Else there is unmatchable material here.\n",
        "                # It ends where a skippable or token match starts, or at the end of the line.\n",
        "                unmatchable_end = len(line)\n",
        "                if skippable_match:\n",
        "                    unmatchable_end = skippable_match.start()\n",
        "                if token_match:\n",
        "                    unmatchable_end = min(unmatchable_end, token_match.start())\n",
        "                # Add it to unmatchable and discard from line.\n",
        "                unmatchable.append(line[:unmatchable_end])\n",
        "                line = line[unmatchable_end:]\n",
        "\n",
        "    return tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#ex_tweet = DATA['hate']['train_text'][0]\n",
        "ex_tweet = \"https://t.co/9z2J3P33Uc FB needs to hurry up and add a laugh/cry button 😬😭😓🤢🙄😱 Since eating my feelings has not fixed the world's problems, I guess I'll try to sleep... HOLY CRAP: DeVos questionnaire appears to include passages from uncited sources https://t.co/FNRoOlfw9s well played, Senator Murray Keep the pressure on: https://t.co/4hfOsmdk0l @datageneral thx Mr Taussig It's interesting how many people contact me about applying for a PhD and don't @user spell my name #something right.\"\n",
        "\n",
        "tokeniser = [tokeniser1, tokeniser2, tokeniser3]\n",
        "\n",
        "print('Original Tweet: ',ex_tweet, '\\n')\n",
        "for i in range(len(tokeniser)):\n",
        "    print(f\"Tokeniser {i+1}: {tokeniser[i](ex_tweet)}\")"
      ]
    },
    {
      "source": [
        "### Summary\n",
        "---\n",
        "\n",
        "Tokeniser 1 (Finds all Words):\n",
        "+ Good representation of words (includes Unicode characters and is sensible to punctuation within words, ie. \"don't\")\n",
        "- Deletes all punctuation (which might be important, ie. twitter communication heavily based tagging using '@' and hastags (#))\n",
        "- Destroys Links\n",
        "- Blind to Emojis\n",
        "\n",
        "Tokeniser 2 (Split at Whitespace):\n",
        "+ Overall good representation of words \n",
        "+ keeps punctuation\n",
        "- ending punctuation is not being separated\n",
        "+ keeps Links\n",
        "+ recognises Emojis\n",
        "- doesnt separate adjacent emojis \n",
        "- blind to several words written together (in slang), ie. idontgiveafuck\n",
        "\n",
        "Tokeniser 3 (Twitter Tokeniser API, Overcomes issues of Tokeniser 2):\n",
        "+ Ending punctuation is being separated\n",
        "+ Separate adjacent Emojis "
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "*TASK 2*\n",
        "# XXX\n",
        "---"
      ],
      "metadata": {
        "tags": [],
        "cell_id": "00012-f7ed8c9e-4e5c-4499-8cbd-bc465deeb219",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# some code"
      ],
      "metadata": {
        "tags": [],
        "cell_id": "00015-c0d67151-ed60-49d4-822d-0806e71831bc",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "*TASK 3*\n",
        "# Open Question: XXX\n",
        "---"
      ],
      "metadata": {
        "tags": [],
        "cell_id": "00013-fc964c9b-2d90-48f7-93b6-89e1c191c669",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# some code"
      ],
      "metadata": {
        "tags": [],
        "cell_id": "00017-2bcdfccd-1c4e-4b00-abcb-2807c0f7da9c",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "orig_nbformat": 2,
    "deepnote": {
      "is_reactive": false
    },
    "deepnote_notebook_id": "912c8bda-48f1-43bf-8bff-0aac9dd9fbcb",
    "deepnote_execution_queue": [],
    "kernelspec": {
      "name": "python383jvsc74a57bd0ce483ef3f3f15830cc71af6662324550274ded09d10a1a693bdaad1eb103022d",
      "display_name": "Python 3.8.3 64-bit ('base': conda)"
    }
  }
}