{
 "cells": [
  {
   "cell_type": "markdown",
   "source": "# Project 4: Natural Language Processing \n---\n\n**Group 9: Aidan Stocks, Hugo Reinicke, Nicola Clark, Jonas-Mika Senghaas**\n\nSubmission: *03.06.2021* / Last Modified: *02.06.2021*\n\n---\n\nThis notebook contains the step-by-step process of building a natural language machine learning model to automatically detect *(a)* Hatespeech and *(b)* Emotion in tweets on the social network Twitter. \n\nThe initial data was obtained from the [TweetEval](https://github.com/cardiffnlp/tweeteval#evaluating-your-system) GitHub repository, that provides data for supervised training of classifiers for natural language processing, more specifically it provides prepared data for several mini-project involving the analysis of different characteristics of tweets.",
   "metadata": {
    "cell_id": "00000-8f8eb98f-2a98-41af-9825-5a8a8c441dee",
    "tags": [],
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "markdown",
   "source": "## Introduction\n---\nSocial media is omnipresent in today's world. We use messengers to communicate, share pictures, music, thoughts - in short - our life on the internet with people that are close, and maybe also not as close to us. Twitter is one of those social networks. The american social networking service allows its users to post and interact with messages known as through so-called tweets. 280 character postings on the online-service that can be liked, commented, threaded and shared. Since its launch in 2006, *Twitter* has grown massively, nowadays reporting hundreds of million of users. Besides its diverse utilisation, Twitter is especially known for a platform for political discussion. Both politicians and society use Twitter as a channel to take positions in politcal debates and express opinion. \n\nWhile this is desirable and embracing the idea of free-speech on the internet, the question of whether or not Twitter should use tools to automatically detect unwanted content from its platform, such as racism, sexism, false information or hatespeech, is a subject of on-going public debate. \n\nThis project, in a first instance, sets aside the ethical challenges and questions arising, and solely focuses on the technical details of how such a solution might work. The goal of this project is to optimise a machine learning model to automatically detect unwanted content.",
   "metadata": {
    "cell_id": "00001-5fcfa044-05ac-485a-8293-0db98813ff5e",
    "tags": [],
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "markdown",
   "source": "## Running this Notebook\n---\nThis notebook contains all code to reproduce the findings of the project as can be seen on the [GitHub](https://github.com/jonas-mika/fyp2021p04g09) page of this project. In order to read in the data correctly, the global paths configured in the section `Constants` need to be correct. The following file structure - as prepared in the `submission.zip` - was followed throughout the project and is recommended to use (alternatively the paths in the section `Constants` can be adjusted):\n\n```\n*project tree structure*\n```\n*Note that the rest of the file structure as can be seen on the [GitHub](https://github.com/jonas-mika/fyp2021p03g09) page of the project generates automatically*",
   "metadata": {
    "cell_id": "00002-e3f8c8e5-1dc7-47ff-9108-2450997c051c",
    "tags": [],
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "markdown",
   "source": "## Required Libraries and Further Imports\n---\nThroughout the project, we will use a range of both built-in and external Python libraries. This notebook will only run if all libraries and modules are correctly installed on your local machines. \nTo install missing packages use `pip install <package_name>` (PIP (Python Package Index) is the central package management system, read more [here](https://pypi.org/project/pip/)). \n\nIn case you desire further information about the used packages, click the following links to find detailed documentations:\n- [Pandas](https://pandas.pydata.org/)\n- [Numpy](https://numpy.org/)\n- [Matplotlib](https://matplotlib.org/stable/index.html)\n- [SciKit Learn](https://scikit-learn.org/stable/)\n- [NLTK](https://www.nltk.org/)",
   "metadata": {
    "cell_id": "00003-8f195f8a-4025-446a-8b44-300b55666211",
    "tags": [],
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00004-cf13b571-fc4d-40f4-8617-43224164fcb8",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 10203,
    "execution_start": 1622625048986,
    "output_cleared": true,
    "source_hash": "2841754f",
    "tags": [],
    "deepnote_cell_type": "code"
   },
   "source": "%%capture\n# uncomment lines with uninstalled packages\n\n#!pip install -U numpy pandas matplotlib seaborn skikit-learn \n!pip install pycontractions\n!pip install imblearn\n!pip install statsmodels",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00005-9cbe0305-7f30-486b-be81-0c10af861246",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 67,
    "execution_start": 1622640005481,
    "output_cleared": true,
    "source_hash": "f245782b",
    "tags": [],
    "deepnote_cell_type": "code"
   },
   "source": "# python standard libraries\nimport json                                            # data transfer to and from json format\nimport os                                              # access operating system from python\nimport math                                            # mathematical operations in python\nimport random                                          # creates randomness\nimport re                                              # regex search in python\nimport shutil                                          # system control in python\nimport warnings                                        # ignore annoying warnings\nwarnings.filterwarnings(\"ignore\")\n\n# external libraries\nimport numpy as np                                     # used for numerical calculations and fast array manipulations\nimport pandas as pd                                    # provides major datastructure pd.DataFrame() to store datasets\nimport matplotlib\nimport matplotlib.pyplot as plt                        # basic data visualisation\nfrom matplotlib.ticker import MaxNLocator\nimport seaborn as sns                                  # advanced data visualisation\nfrom nltk.tokenize import TweetTokenizer               # tokeniser api\nfrom nltk.corpus import stopwords\nnltk.download('stopwords');\nfrom pycontractions import Contractions                # intelligently expands contractions in natural language\nfrom collections import Counter                        # counts objects",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# feature extraction\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer # not used yet\n\n# addressing imbalance in data\nfrom imblearn.over_sampling import RandomOverSampler as ROS\n\n# classifiers\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\n# performance evaluation\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import precision_recall_fscore_support\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report",
   "metadata": {
    "tags": [],
    "cell_id": "00006-16d5cbf8-01cc-41a9-9fa2-dbbfdc49a2d0",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "bddaa275",
    "execution_start": 1622640007578,
    "execution_millis": 1,
    "output_cleared": true,
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00006-5afa390e-a19c-4f7b-b2f5-752fba00777d",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 43,
    "execution_start": 1622620761466,
    "output_cleared": true,
    "source_hash": "9a25b5af",
    "deepnote_cell_type": "code"
   },
   "source": "print(f'Numpy Version: {np.__version__}')\nprint(f'Pandas Version: {pd.__version__}')\nprint(f'Matplotlib Version: {matplotlib.__version__}')\nprint(f'Seaborn Version: {sns.__version__}')",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Constants\n---\nTo enhance readibilty, as well as to decrease the maintenance effort, it is useful for bigger projects to define contants that need to be accessed globally throughout the whole notebook in advance. \nThe following cell contains all of those global constants. By convention, we write them in caps (https://www.python.org/dev/peps/pep-0008/#constants)",
   "metadata": {
    "cell_id": "00008-4f347db9-f433-4a57-9e83-4fbb27586617",
    "tags": [],
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00010-18532f0a-c77d-42e2-b487-aa327b69dc04",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 0,
    "execution_start": 1622626358206,
    "output_cleared": true,
    "source_hash": "74c0338e",
    "tags": [],
    "deepnote_cell_type": "code"
   },
   "source": "PREPROCESS_DATA = False\nGENERATE_IAA_SAMPLE = False",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "random.seed(1)",
   "metadata": {
    "tags": [],
    "cell_id": "00009-d56d40bc-118b-4ddd-b029-25d968bcf22d",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "52cd5b5a",
    "execution_start": 1622620761505,
    "execution_millis": 1841609,
    "output_cleared": true,
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00009-8c033e56-02ab-48b7-9eff-1efefe08d35c",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 1,
    "execution_start": 1622621373851,
    "output_cleared": true,
    "source_hash": "d3d11c4b",
    "tags": [],
    "deepnote_cell_type": "code"
   },
   "source": "DATASETS = ['hate', 'emotion']\n\n# store paths\nPATH = {}\nPATH['data'] = {}\nPATH['data']['raw'] = \"../data/raw/\"\nPATH['data']['processed'] = \"../data/processed/\"\n\n# store data \nDATA = {}\nDATA['raw'] = {}\nDATA['processed'] = {}\nfor dataset in DATASETS:\n    DATA['raw'][dataset] = {}\n    DATA['processed'][dataset] = {}\n\nCORPUS = {}\nVOCABULARY = {}",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# Fetching Data\n---\n*TASK 0*\n\nAfter having setup the overall scheme of the project, we need to fetch the data we want to work on. The datasets were obtained from the\n[TweetEval](https://github.com/cardiffnlp/tweeteval#evaluating-your-system) GitHub repository, which provides ready-to-work on tweets and gold-standard annotations for different focuses of analysis - already split up into training, validation and testing sets.\n\n1. Binary Classifcation: **Hate-Speech**\n > Two Labels: *Hate-Speech*/ *Not Hate-Speech*\n\n > [Raw Data (GitHub)](https://github.com/cardiffnlp/tweeteval/tree/main/datasets/hate)\n\n2. Multiclass Classification: **Emotion Recognition**  \n >Four Labels: *Anger*, *Joy*, *Sadness*, *Optimism* \n\n >[Raw Data (GitHub)](https://github.com/cardiffnlp/tweeteval/tree/main/datasets/emotion)\n\n*Note, in order to fetch this data into the Jupyter, the above-mentioned files need to be in the existent in the file structure and the location specified in the code and file tree structure in the introductory section. This should be the case in the submission and if this project was pulled or forked directly from [GitHub](https://github.com/jonas-mika/fyp2021p04g09)* ",
   "metadata": {
    "cell_id": "00010-30f29e03-2c93-4a41-9cdd-c63ddcf81385",
    "tags": [],
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "markdown",
   "source": "## Loading in Data\n---\nWe start off by loading in the data obtained from the above-mentioned sources into the script. All tweets are being read into a list of strings, where each string is representing a single tweet. The golden labels are being read into an index-corresponding array of integers, where the integer at *i*th position is the gold label for the tweet in the list of tweets at position *i*. Lastly, we read in the mapping, between the integer and the corresonding label into a dictionary, such that we can use it for nice plotting.",
   "metadata": {
    "cell_id": "00012-517a92e6-641e-4b58-ae16-4fc3f1dc0f0c",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00011-d60e995b-fcb6-4868-b5f3-bead2a4d235a",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 0,
    "execution_start": 1622621594269,
    "output_cleared": true,
    "source_hash": "e386fa8c",
    "tags": [],
    "deepnote_cell_type": "code"
   },
   "source": "# helper function to read in raw files correctly\ndef read_raw_input(dataset):\n    # reading in all .txts into list of strings\n    for _file in os.listdir(f'../data/raw/{dataset}'):\n        with open(f'../data/raw/{dataset}/{_file}', 'r', encoding='UTF-8') as infile:\n            DATA['raw'][dataset][_file[:-4]] = [line.strip() for line in infile.readlines()]\n\n    # convert target labels to integers\n    for key in ['train_labels', 'val_labels', 'test_labels']:\n        DATA['raw'][dataset][key] = [int(x) for x in DATA['raw'][dataset][key]]\n\n    # convert mapping to dictionary\n    DATA['raw'][dataset]['mapping'] = {int(string.split('\\t')[0]): string.split('\\t')[1] for string in DATA['raw'][dataset]['mapping']}",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00014-a564b5fd-9112-40ea-ab20-a9e06cef672c",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 28,
    "execution_start": 1622621596613,
    "output_cleared": true,
    "scrolled": false,
    "source_hash": "32dcc143",
    "deepnote_cell_type": "code"
   },
   "source": "# read in the the raw data as specified above for both the hate and emotion data\nfor dataset in DATASETS:\n    read_raw_input(dataset)",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Exploring Data\n---\nLet's get a feeling for what kind of data we are dealing with. For now, we simply peek into our actual data, the tweets, and output them with their corresponding gold label. After that, we plot the distribution of the labels, which is an important characteristic of the training process, since it might reveal possible imbalances that need to be addressed before training a model.",
   "metadata": {
    "cell_id": "00015-6ac2c66d-4d76-4711-82ed-29c0f25e3314",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "markdown",
   "source": "### Data Size\n---\nBefore starting to tackle the specific data, we should get a feeling with what size of data we are dealing with. Especially in NLP this is an important step, since NLP models usually incorporate high amounts of data in order to build well-performing models.",
   "metadata": {
    "tags": [],
    "cell_id": "00016-304c396c-0e65-4df0-947d-87cee089d03f",
    "deepnote_cell_type": "markdown"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "!ls -lh ../data/raw/hate",
   "metadata": {
    "tags": [],
    "cell_id": "00017-ac745fa2-4c9e-4367-8bde-fb896f797948",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "8c443e46",
    "execution_start": 1622621037812,
    "execution_millis": 675,
    "output_cleared": true,
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "!ls -lh ../data/raw/emotion",
   "metadata": {
    "tags": [],
    "cell_id": "00018-bb627de0-990f-4914-acb7-a5f7583e5420",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "a31cd803",
    "execution_start": 1622621034399,
    "execution_millis": 700,
    "output_cleared": true,
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "As can be seen, the file sizes are considerably small for NLP datasets. This means, that we most likely will be able to work with the data on local memory. However, we are trying to follow best practice and work RAM efficient during the project.",
   "metadata": {
    "tags": [],
    "cell_id": "00019-0358f284-28d9-49ce-b191-7919bcfad280",
    "deepnote_cell_type": "markdown"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "!wc -l ../data/raw/hate/*_text.txt",
   "metadata": {
    "tags": [],
    "cell_id": "00020-f3ee83e4-95f6-4de1-a828-697178c1a0b7",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "d8f1efc2",
    "execution_start": 1622621186027,
    "execution_millis": 691,
    "output_cleared": true,
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "!wc -l ../data/raw/emotion/*_text.txt",
   "metadata": {
    "tags": [],
    "cell_id": "00021-bc546d23-a932-4068-abac-00a0132e9d23",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "40594db",
    "execution_start": 1622621181707,
    "execution_millis": 714,
    "output_cleared": true,
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "The `hate` dataset contains more than double the tweets than the `emotion` dataset. However, the split is rather similar in both instances with a roughly 70-20-10 split for training, testing and validation.",
   "metadata": {
    "tags": [],
    "cell_id": "00022-4651d104-1cb3-4f44-a9fc-3ef8673a049d",
    "deepnote_cell_type": "markdown"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "### Visualising Raw Data\n---",
   "metadata": {
    "cell_id": "00016-1ade1bb5-34a5-4a97-9049-49dbb00ee154",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00017-8c456ad3-3f69-4ec0-931f-2fdc6fe0e33e",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 10,
    "execution_start": 1622620761591,
    "output_cleared": true,
    "source_hash": "4847394d",
    "deepnote_cell_type": "code"
   },
   "source": "# hate\nfor i in range(10):\n    print(f\"{i+1}\\tLabel: {DATA['raw']['hate']['mapping'][DATA['raw']['hate']['train_labels'][i]].title()}\\t\\t{DATA['raw']['hate']['train_text'][i]}\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "The first ten tweets from the training set of the hatespeech task look as expected. Tweets are a single-line string, including emojis, syntax, grammar and spelling of the orginal tweet. Both the author of the tweet and links to other users, however, are anonymised for data privacy reasons. \n\nThe gold standard labels appear reasonable for the first ten tweets.",
   "metadata": {
    "tags": [],
    "cell_id": "00017-8efa9570-f2fb-44cd-bdb4-33f75960603e",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00018-f6587a74-f6da-4c6c-8540-db447f5615d9",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 1926130,
    "execution_start": 1622620761634,
    "output_cleared": true,
    "source_hash": "c303da21",
    "deepnote_cell_type": "code"
   },
   "source": "# emotion\nfor i in range(10):\n    print(f\"{i+1}\\tLabel: {DATA['raw']['emotion']['mapping'][DATA['raw']['emotion']['train_labels'][i]].title()}\\t\\t{DATA['raw']['emotion']['train_text'][i]}\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "The data for the emotion recognition looks similar. Again, tweets are a single-line string, including emojis, syntax, grammar and spelling of the orginal tweet. Both the author of the tweet and links to other users are anonymised for data privacy reasons. \n\nAlso here, the gold standard labels appear reasonable for the first ten tweets.",
   "metadata": {
    "tags": [],
    "cell_id": "00019-aca6c04c-c1b1-43a8-a6d4-916bedefb0ac",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "markdown",
   "source": "### Visualising Label Distribution\n---\nVisualising the distribution of the target variable in any classification task is an important first step in order to evluate the quality of the data we are training on. Ideally, we would like to observe equally balanced labels, since this prevents models from being overly biased by always predicting a dominant labels. ",
   "metadata": {
    "cell_id": "00019-ac0093ee-cb8e-4df0-9240-28b8639da8f0",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00020-ff2b603d-7d95-43b5-8411-7c54ff2985ec",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 1924662,
    "execution_start": 1622620761635,
    "output_cleared": true,
    "source_hash": "56ff2033",
    "deepnote_cell_type": "code"
   },
   "source": "# helper function to plot the distribution of labels in all splits of the data\ndef visualise_label_distribution(dataset):\n    # initalise figure with three axes\n    fig, ax = plt.subplots(ncols=3, figsize=(12,4))\n    fig.suptitle(f'Frequency of Target Label in {dataset.capitalize()}', fontsize=12, fontweight='bold') # global title\n\n    # plot barplot of label distribution with some additional aesthetic adjustment in each split of the data\n    for i, key in enumerate(['train_labels', 'val_labels', 'test_labels']):\n        label, count = np.unique(DATA['raw'][dataset][key], return_counts=True)\n        ax[i].bar(label, count, color='grey');\n        ax[i].set_title(key.replace('_', ' ').title())\n        ax[i].set_xticks(label); ax[i].set_xticklabels([string.title() for string in DATA['raw'][dataset]['mapping'].values()])",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00021-046d0488-77dc-47d3-a29c-d178ea704a1f",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 422,
    "execution_start": 1622620761635,
    "output_cleared": true,
    "source_hash": "894be1c8",
    "deepnote_cell_type": "code"
   },
   "source": "visualise_label_distribution(dataset='hate')",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "The binary label *Hate* (42%) or *Non-Hate* (58%) seems reasonably balanced. Here, balancing of the labels before building the model is possible, but not mandatory. It furthermore becomes obvious that the label ratio is consistent across all splits (training, validation and testing set).",
   "metadata": {
    "tags": [],
    "cell_id": "00023-e661d70e-826c-4fd1-985d-a284c26e7f7e",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00022-83c0ec2d-8094-4123-8603-caad77edb2af",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 447,
    "execution_start": 1622620762051,
    "output_cleared": true,
    "source_hash": "c0926180",
    "deepnote_cell_type": "code"
   },
   "source": "visualise_label_distribution(dataset='emotion')",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "However, in the multi-class classifcation task we observe a high imbalance. With only 9% of the total amount of data in each split, the label *optimism* is underrepresent, while the label *anger* (43%) is overrepresented. This imbalance needs to be addressed before training our classifiers. \nAgain, however, we observe a consistent label ratio across all splits. ",
   "metadata": {
    "tags": [],
    "cell_id": "00025-fb419f6b-91d7-4063-b2b8-e7462b3b68f1",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "markdown",
   "source": "# Processing of Language Data\n---\n*TASK 1*\n\nNatural language is a highly-complex type of data, that needs to be processed before it can be implented meaningfully into a classification task. For natural language classifiers the main step of processing the raw data - the original tweets - is called **tokenisation**.\n\nOn a high level, computers stand no chance in understanding language. After all, each and every tweet we are dealing with is just a unique concatenation of 0's and 1's, that in itself is useless for classifying the tweet in any way. We must therefore transform our data in a way that our computer is able to understand it. This is done during the process of tokenisation in natural language processing. Tokenisation in NLP generally means the concept of breaking down the original data into smaller parts, called tokens. Those tokens could be sentences, one or more words, or even smaller chunks of text. Either way, they are used to built a vocabulary that ultimately results in a count vector machine, that serves as input to the machine learning model.",
   "metadata": {
    "cell_id": "00011-c1d3e256-83ab-4833-93bb-155c6dcb73ca",
    "tags": [],
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "markdown",
   "source": "## Building Tokeniser\n---\nIn this project we chose to build a word-tokeniser, meaning that the ideal tokeniser recognises words and splits accordingly. However, due to the fact that we are working with tweet data, there are some special requirements in order to get optimal performance in our models. \n\n**Features of Tokeniser**\n\n1. Lowercase ()\n\n2. Links (Links are a core part of a tweet containing it - if not the core. To not lose information, we must include the links in our classification. However, for the classfication the actual link address is of minor importance. For this reason, we would like to recognise any link, then replace it with some dummy string that the model uses to classify any kind of link in a tweet.\n\n3. Emojis (Emojis have developed to a central method of communicating emotions. It is therefore important that our tokeniser recognises single emojis and feeds them into our classifier.)\n\n4. Hashtags ()\n\n4. Contractions (Has not or Hasn't? It means the same, so we would like our model to recognise the two as the same token.)\n",
   "metadata": {
    "cell_id": "00024-925deca3-bb20-43eb-bc18-98e47d7a5780",
    "tags": [],
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "markdown",
   "source": "The following code cells contain how the tokeniser was built in this project. It is fundamentally based on the *Tweet Tokeniser* provided by the NLTK library and then adds additional functionality using both RegEx and another extyernally trained model for tokenising contractions correctly.",
   "metadata": {
    "tags": [],
    "cell_id": "00028-9ffd5945-a956-4a50-ae3e-2a5bafdbc89d",
    "deepnote_cell_type": "markdown"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# loading pre-trained contraction model\nif PREPROCESS_DATA == True:\n    cont.load_models() # doesn't work on deepnote, because of memory issues. works locally\n\ncont = Contractions(api_key=\"glove-twitter-200\")",
   "metadata": {
    "tags": [],
    "cell_id": "00028-e796ec1e-9733-4523-b144-52042e24ba94",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "e6ca8f6a",
    "execution_start": 1622620762493,
    "execution_millis": 20,
    "output_cleared": true,
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00043-2f55d04c-3946-4a00-a740-a8debdffb56c",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 17,
    "execution_start": 1622620762520,
    "output_cleared": true,
    "source_hash": "ba5dc7cf",
    "tags": [],
    "deepnote_cell_type": "code"
   },
   "source": "# define url regex pattern and replace pattern globally (compile for speedup)\nurl_pattern = re.compile(r\"(?i)((?:https?://|www\\d{0,3}[.]|[a-z0-9.-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|(([^\\s()<>]+|(([^\\s()<>]+)))))+(?:(([^\\s()<>]+|(([^\\s()<>]+))))|[^\\s`!()[]{};:'\\\".,<>?«»\\“\\”‘’]))\")\ndummy = \" <=LINK=> \"\n\n# brush up functions\ndef remove_links(tweet):\n    return re.sub(url_pattern, dummy, tweet) # substituting dummy for every link\n\ndef remove_newline(tweet):\n    return re.sub(r'\\\\n', ' ', tweet) # deals with issue of newline characters within tweets\n\ndef expand(tweet):\n    return list(cont.expand_texts([tweet]))[0] # expands contractions",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "tk = TweetTokenizer()\n\ndef tokenise(tweet):\n    cleaned_tweet = expand(remove_links(remove_newline(tweet.lower())))\n    return tk.tokenize(cleaned_tweet)",
   "metadata": {
    "tags": [],
    "cell_id": "00031-69719e76-f2e4-4188-b5ef-2c510e3a7879",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "1b22e0bb",
    "execution_start": 1622620762546,
    "execution_millis": 16,
    "output_cleared": true,
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## Exporting Tokenised Tweets\n---\nWe have built the final tokeniser that will be used throughout this project. To not stretch out the RAM, we iterate over the raw data tweet by tweet, tokenise it and write it into an external file. The final tokenised tweets will be separated by tabs, such that they can easily be read back in again, while keeping the split between tokens. \n\nFor both datasets, all processed files will be in saved into a subfolder called `hate` or `emotion` within the folder `processed` in `data`. While exporting, we maintain the naming convention of the raw data. This enables us to use the same function to read in the processed data again. \n\n*Note that directories are created automatically and thus do not need to be created by hand.*",
   "metadata": {
    "tags": [],
    "cell_id": "00032-066aa9d6-3735-4942-af9d-ed96974f73ed",
    "deepnote_cell_type": "markdown"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00046-8f2c5b62-9cce-4460-a430-78de3377ebca",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 3,
    "execution_start": 1622620762721,
    "output_cleared": true,
    "source_hash": "be8da45",
    "tags": [],
    "deepnote_cell_type": "code"
   },
   "source": "if PREPROCESS_DATA == True:\n    for dataset in DATASETS:\n        # create preprocessed folder\n        try: os.makedirs(f'../data/processed/{dataset}')\n        except: None\n\n        # preprocess and tokenize tweets\n        for key in ['train_text', 'val_text', 'test_text']:\n            with open(f'../data/processed/{dataset}/{key}.txt', 'w', encoding = 'UTF-8') as outfile:\n                for tweet in DATA['raw'][dataset][key]:\n                    outfile.write('\\t'.join(tokenise(tweet)) + '\\n') # writing tokenised tweet with tab delimiter\n\n        # copy labels and mapping without preprocessing\n        for key in ['train_labels', 'val_labels', 'test_labels', 'mapping']:\n            shutil.copyfile(f'../data/raw/{dataset}/{key}.txt', f'../data/processed/{dataset}/{key}.txt')",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00042-632ac15c-7d49-4503-b780-5829b6f5faac",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 90,
    "execution_start": 1622621604293,
    "output_cleared": true,
    "source_hash": "e8fcf339",
    "tags": [],
    "deepnote_cell_type": "code"
   },
   "source": "# loading in processed data\nfor dataset in DATASETS:\n    for key in os.listdir(f'../data/processed/{dataset}'):\n        with open(f'../data/processed/{dataset}/{key}', 'r', encoding='UTF-8') as infile:\n            DATA['processed'][dataset][key[:-4]] = [line.strip().split('\\t') for line in infile.readlines()]\n\n    # convert target labels to integers\n    for key in ['train_labels', 'val_labels', 'test_labels']:\n        DATA['processed'][dataset][key] = [int(x) for x in [DATA['processed'][dataset][key][i][0] for i in range(len(DATA['processed'][dataset][key]))]]\n\n    # convert mapping to dictionary\n    DATA['processed'][dataset]['mapping'] = {int(string[0]): string[1] for string in DATA['processed'][dataset]['mapping']}",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Testing Tokeniser\n---\nThe tokeniser is built and has been applied onto the entire corpus. Let's evaluate its performance using the same first ten tweets as we peeked into in the first section.",
   "metadata": {
    "tags": [],
    "cell_id": "00032-b6926ea2-8ec4-41b7-9bc5-f7558b5bb006",
    "deepnote_cell_type": "markdown"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "for i in range(10):\n    print(f\"Original Tweet: {DATA['raw']['hate']['train_text'][i]}\\nTokenised Tweet: {'-'.join(DATA['processed']['hate']['train_text'][i])}\\n\")",
   "metadata": {
    "tags": [],
    "cell_id": "00033-ee786581-47b7-48de-b8cf-a5f5a7c848dd",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "e752b4be",
    "execution_start": 1622620762870,
    "execution_millis": 19,
    "output_cleared": true,
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "for i in range(10):\n    print(f\"Original Tweet: {DATA['raw']['emotion']['train_text'][i]}\\nTokenised Tweet: {'-'.join(DATA['processed']['emotion']['train_text'][i])}\\n\")",
   "metadata": {
    "tags": [],
    "cell_id": "00038-66f6ec60-1a52-4324-91e7-4d0dd06c50e3",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "b23e6d6a",
    "execution_start": 1622620762909,
    "execution_millis": 717132,
    "output_cleared": true,
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "In both small samples from the two datasets the tokeniser seems to perform well. As expected, we split the tweets by words. Furthermore, the tokeniser is able to keep hashtags, expand contractions, deal correctly with emojis and replace links with the dummy string.\nWe can now continue our work based on the tokenised data.",
   "metadata": {
    "tags": [],
    "cell_id": "00040-72d80d21-7124-4b56-945d-dc1d4d75819b",
    "deepnote_cell_type": "markdown"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "# Descriptive Statistics\n---\n*TASK 2* \n\nAn essential part of any NLP classfication task is to describe the corpus, so the entire tokenised data. In this section, we will step-by-step compute the most important statistics and visualise some of them for both datasets.",
   "metadata": {
    "cell_id": "00044-cef6a6e4-c220-496f-ada8-d6cfbc08f299",
    "tags": [],
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "markdown",
   "source": "## Corpus\n---\nGenerally in linguistics, a corpus is a large and structured set of texts. The corpus is often used as the basis for a lot of descriptive analysis of the language data at hand. We therefore create a corpus for both training sets of our datasets using a flattened list, that is a linear conctentation of all tokens in the training sets of both datasets.",
   "metadata": {
    "tags": [],
    "cell_id": "00042-1e95836c-c9bd-4ece-9172-59d54d3e574e",
    "deepnote_cell_type": "markdown"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00046-c2b2e52f-155f-4bb0-82f8-3eb9d7f2c2c9",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "6f4d3901",
    "execution_start": 1622621806968,
    "execution_millis": 1,
    "output_cleared": true,
    "deepnote_cell_type": "code"
   },
   "source": "# build entire corpus from training set in both datasets\nfor dataset in DATASETS:\n    CORPUS[dataset] = [token for tweet in DATA['processed'][dataset]['train_text'] for token in tweet]",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "for dataset in DATASETS:\n    print(f'Corpus Size ({dataset.title()}): {len(CORPUS[dataset])}')",
   "metadata": {
    "tags": [],
    "cell_id": "00050-8a91ad41-e2f2-46cf-b897-ce337bbf738d",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "fad6c5f0",
    "execution_start": 1622622385059,
    "execution_millis": 14,
    "output_cleared": true,
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "As we can see, the corpus for the `hate` classification is significantly larger with 216.076 total number of tokens than the `emotion` corpus with 60.506 tokens.",
   "metadata": {
    "tags": [],
    "cell_id": "00051-d506cc94-5179-4636-952a-85dbedfbf123",
    "deepnote_cell_type": "markdown"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## Vocabulary \n---\nIn NLP, the set of unique words used in the text corpus is referred to as the vocabulary. The vocabulary is the basis for training machine learning models based on the bag-of-words approach, which treats every unique token as a feature in the classification.\n\nIn order to built the vocabulary, we use the `Counter` class from the `collections` standard library in python onto the corpus, in order to compute a dictionary containing all unique tokens (types) together with their corresponding frequency. We combine all data, together with some additional information like the `rank`, where 1 is the the most frequent token, the `normalised/ relative frequency` (frequency divided by total number of tokens) and the `cumulative frequency`. The final data frame will be sorted in descending order, from most to least frequent tokens.",
   "metadata": {
    "tags": [],
    "cell_id": "00044-839f89a7-e2e8-4f6b-b9b0-effc3f566bb3",
    "deepnote_cell_type": "markdown"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00042-f580c106-03ae-4ef2-a8de-3329cc7f39d7",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 41,
    "execution_start": 1622625729491,
    "output_cleared": true,
    "source_hash": "d2b04462",
    "tags": [],
    "deepnote_cell_type": "code"
   },
   "source": "# build entire vocabulary from training tweets in both datasets\nfor dataset in DATASETS:\n    corpus = CORPUS[dataset]\n\n    # initialise frequency counter\n    VOCABULARY[dataset] = pd.DataFrame.from_dict(Counter(corpus), orient='index').reset_index().rename(columns={'index': 'token', 0: 'frequency'}).sort_values(by='frequency', ascending=False).reset_index().drop(columns=['index'])\n\n    VOCABULARY[dataset]['rank'] = VOCABULARY[dataset].index + 1 # add one to index to build rank\n    VOCABULARY[dataset]['normalised_frequency'] = VOCABULARY[dataset]['frequency'] / VOCABULARY[dataset]['frequency'].sum() # relative frequency\n    VOCABULARY[dataset]['cumulative_frequency'] = VOCABULARY[dataset]['normalised_frequency'].cumsum()",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "Let's have a look at the vocabulary dataframes constructed for both dataframes.",
   "metadata": {
    "tags": [],
    "cell_id": "00054-3ef36aa2-e3f2-404a-a526-c6c1a5e49a7c",
    "deepnote_cell_type": "markdown"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00046-5accac30-2f91-4cf6-8d65-ca5f6e2cfff1",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 97,
    "execution_start": 1622625730749,
    "output_cleared": true,
    "source_hash": "76d6d744",
    "tags": [],
    "deepnote_cell_type": "code"
   },
   "source": "VOCABULARY['hate']",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "VOCABULARY['emotion']",
   "metadata": {
    "tags": [],
    "cell_id": "00055-e83734c0-8d5f-4ade-80c9-de9ab6ebe35e",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "81b7fded",
    "execution_start": 1622625734608,
    "execution_millis": 45,
    "output_cleared": true,
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## Type/ Token Ratio\n---\nThe type/ token ratio of is the quotient of number of types (vocabulary size) to number of tokens (text/corpus size). We compute it in the following way. ",
   "metadata": {
    "tags": [],
    "cell_id": "00054-2a4e34bf-3b29-4b52-9f8e-a095d053a117",
    "deepnote_cell_type": "markdown"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "for dataset in DATASETS:\n    print(f'TTR ({dataset.title()}): {round(len(VOCABULARY[dataset]) / len(CORPUS[dataset]),2)}')",
   "metadata": {
    "tags": [],
    "cell_id": "00056-4199ec48-921d-462c-b7e4-dac4515502fd",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "ec67b3c2",
    "execution_start": 1622625736961,
    "execution_millis": 12,
    "output_cleared": true,
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "A high TTR generally indicates a high degree of lexical variation in the corpus. However, in this context the higher TTR in the emotion dataset might also result from the fact that it is a lot smaller in size than the hate dataset and thus the TTR score is less biased by few very frequently occurring tokens that decrease the TTR score.",
   "metadata": {
    "tags": [],
    "cell_id": "00059-6dc5f797-8118-479c-ace0-8023ac85684c",
    "deepnote_cell_type": "markdown"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## Zipf's Law\n---\nZipf's Law says that the frequency of a word is inversely proportional to its rank in the frequency table. Thus, very few words occur very frequently, while the majority of words have very low frequencies. Tyical frequent words are function words, such as prepositions, pronouns, conjunctions and similar words.\n\nLet's visualise Zipf's Law for both of our datasets.",
   "metadata": {
    "cell_id": "00046-7c9fef28-200d-494f-b96d-c48da9560645",
    "tags": [],
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "source": "def visualise_zipf(dataset, log = False):\n    if log:\n        x = np.log(VOCABULARY[dataset]['rank'])\n        y = np.log(VOCABULARY[dataset]['frequency'])\n    else: \n        x = VOCABULARY[dataset]['rank']\n        y = VOCABULARY[dataset]['frequency']\n\n    fig, ax = plt.subplots(ncols=2, figsize=(15,6))\n    ax[0].plot(x, y);\n    ax[0].set_xlabel('Rank'); ax[0].set_ylabel('Frequency'); ax[0].set_title(f'Frequency of Tokens in {dataset.title()}'); \n    ax[1].plot(VOCABULARY[dataset]['rank'], VOCABULARY[dataset]['cumulative_frequency']);\n    ax[1].set_xlabel('Rank'); ax[1].set_ylabel('Cumulative Frequency'); ax[1].set_title(f'Cumulative Frequency of Tokens in {dataset.title()}'); ",
   "metadata": {
    "tags": [],
    "cell_id": "00061-03a45457-c697-453e-9402-fbded2617f28",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "df912ae0",
    "execution_start": 1622626041586,
    "execution_millis": 0,
    "output_cleared": true,
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "visualise_zipf(dataset='hate', log=False)",
   "metadata": {
    "tags": [],
    "cell_id": "00062-1721443e-c444-402c-9ee9-01958b97b2a5",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "6b40ea25",
    "execution_start": 1622638786883,
    "execution_millis": 430,
    "output_cleared": true,
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "visualise_zipf(dataset = 'emotion', log = False)",
   "metadata": {
    "tags": [],
    "cell_id": "00063-2fc1f4a5-1fb3-4915-862b-b50821204c15",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "f524469b",
    "execution_start": 1622626046807,
    "execution_millis": 393,
    "output_cleared": true,
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "# Digression: N-Gram Model\n---\nN-Gram Language models are an important tool in NLP, i.e. used in the context of auto-completion or tools like spell checkers. The idea of the model is rather simple: Given a sequence of words, predict the next word that appears with highest probability. In that sense, N-Gram models are a way of finding the maximum likelihood word after a sequence of previous words. But, how do we find this maximum likelihood word? From the point of probability theory, the probability of a word occuring is the conditional probability of seeing some word, given that we already know that that a sequence of previous words has occurred. However, calculating those probabilites for bigger coropora is computationally infeasible. \n\nHowever, one can simplify the computation. The *k*th order *Markov assumption* states that each element of a sequence of tokens only depends on the *k* immediately preceding elements, such that manual \n\n$$\np(w_i|w_{1}, ...,w_{i-1}) \\approx (w_i|w_{i-k}, ..., w_{i-1})\n$$\n\n",
   "metadata": {
    "cell_id": "00012-f7ed8c9e-4e5c-4499-8cbd-bc465deeb219",
    "tags": [],
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "markdown",
   "source": "*Note: For exploring n-gram models we are using the training set of the `hate` dataset.*",
   "metadata": {
    "tags": [],
    "cell_id": "00065-24610fd5-4704-4365-87e2-73994c5015df",
    "deepnote_cell_type": "markdown"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## Manual Model Prediction\n---\nBefore using external libraries to implement n-gram models, we wanted to give it a shot and code it out for ourselves in the simplest of all cases, the bigram. In this simple model we iterate over each tokenised tweet in the data pairwise, and while doing so incrementing the counter of seeing word 2, given that we have seen word 1 in a nested dictionary. We can then easily convert those counts into conditional probability and from that choose the word that is maximally likely give some word. A `generate` function is then the natrual extension that consecutively chooses the next most likely word for a sequence of *n* words.",
   "metadata": {
    "tags": [],
    "cell_id": "00065-aac8fc30-0b52-4539-85c5-aabb56acac2d",
    "deepnote_cell_type": "markdown"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "from nltk import bigrams\nfrom collections import Counter, defaultdict\n\nclass OurBigram:\n    def __init__(self):\n        # initialise empty skeleton structure of model as nested dictionary (initialise all counts to 0)\n        self.model = defaultdict(lambda: defaultdict(lambda: 0))\n    \n    def fit(self, data):\n        # count frequency of co-occurances\n        for sentence in data:\n            for w1, w2 in bigrams(sentence, pad_right=True, pad_left=True):\n                self.model[w1][w2] += 1\n            \n        # transform counts to conditional probabilities\n        for w1 in self.model:\n            total_count = float(sum(self.model[w1].values()))\n            for w2 in self.model[w1]:\n                self.model[w1][w2] /= total_count\n\n    def find_mle(self, word):\n        # for a given word return the word that occurs next with maximum likelihood\n        return max(self.model[word], key=self.model[word].get)\n            \n    def generate(self, word, n):\n        # generate a sequence of n words given an initial word from the bigram model\n        while n >= 0:\n            print(word, end=' ') \n            mle = self.find_mle(word)\n            word = mle\n            n-=1\n\n\nbigram = OurBigram()\nbigram.fit(DATA['processed']['hate']['train_text'])\nbigram.generate('hey', 4)",
   "metadata": {
    "tags": [],
    "cell_id": "00068-60a904b8-bb2c-44fc-8639-22b48bfe3199",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "4c272fef",
    "execution_start": 1622639225133,
    "execution_millis": 215,
    "output_cleared": true,
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## NLTK Language Model\n---\nIn this section, we are exploring externally implemented language models from the NLTK library that we assume ot perform overall better, i.e. in the way it deals with unseen tokens. The following lines of code show, how the model is being trained on hte training set of the `hate` data. We finally generate a sample tweet of length ten.",
   "metadata": {
    "tags": [],
    "cell_id": "00068-265e4a33-9227-4fbd-a4c2-713f97a31626",
    "deepnote_cell_type": "markdown"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00058-8d948cde-2e02-4c37-bc16-fbe8682c2d39",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 5,
    "execution_start": 1622632104934,
    "source_hash": "da652d23",
    "tags": [],
    "output_cleared": true,
    "deepnote_cell_type": "code"
   },
   "source": "# imports for n-gram language model\nfrom nltk.lm.preprocessing import padded_everygram_pipeline\nfrom nltk.lm import MLE",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00062-8d124715-50b4-43a6-9d89-ea885342ff27",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 14255,
    "execution_start": 1622632105685,
    "source_hash": "41955566",
    "tags": [],
    "output_cleared": true,
    "deepnote_cell_type": "code"
   },
   "source": "# number of grams\nn=5\n\n# split \ntrain, vocab = padded_everygram_pipeline(n, DATA['processed'][\"hate\"][\"train_text\"])\n\n# initialise and fit data\nlm = MLE(n)\nlm.fit(train, vocab)",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00076-99664b8f-9d0d-4370-932f-da7947884f7b",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 11,
    "execution_start": 1622639395806,
    "source_hash": "ef730c3",
    "tags": [],
    "output_cleared": true,
    "deepnote_cell_type": "code"
   },
   "source": "'hitler ' + ' '.join(lm.generate(10, text_seed=['hitler'], random_seed=10))",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "We can see that the model is capable of occational, syntactically correct and plausible, hate speech.",
   "metadata": {
    "tags": [],
    "cell_id": "00068-70f85ac3-5810-42eb-bf59-c1448a264cbe",
    "deepnote_cell_type": "markdown"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "# Digression: Manual Annotations and Inter-Annotator Agreement (IAA)\n---\n*TASK 3*\n\nClassification tasks in ML are concerned with predicting a class membership of some object based on computed features. Just as with any other type of objects of data we are dealing with, there are different ways of predicting these labels. \n\n1. **Explicit Rules**: Defining explicit rules in what instances of features, what label to predict\n\n2. **Unsupervised Learning**: Letting the machine find clusters within the data by itself\n\n3. **Supervised Learning**: Manually enriching the data with additional information through annotation.\n\nDefining explicit decision rules is most of the times infeasible for more complex classfication problems. Likewise, unsupervised learning models in NLP often do not give the wanted results. For this reason, the predominant method for knowledge encoding when dealing with natural language is the supervised learning that involves the process of manual annotation. \n\nManually annotating data means to define some kind of *gold standard*, which the label our model ideally predicts. However, this in itself can be a challenge, since language can be ambigious, and interpretation is often subject of personal opinion. \n\nIn this section we therefore manually annotate a small percentage (100 tweets) of the training corpus in the `hate` dataset and compute metrics to what degree we agreed on class membership. We followed the [Annotation Guidelines](https://github.com/msang/hateval/blob/master/annotation_guidelines.md) as employed by the source of the data ([Final Paper](https://www.aclweb.org/anthology/S19-2007.pdf))\n",
   "metadata": {
    "cell_id": "00082-d445badc-5275-4d02-af41-527b34f6b56b",
    "tags": [],
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "markdown",
   "source": "## Generate IAA Sample\n---\nWe first generate the sample of tweets and corresponding gold standard labels as reported by the source of the data. For ease, we chose the last 100 tweets from the training corpus of the `hate` dataset. The tweets were exported into a separate file in the directory `data/annotations`.",
   "metadata": {
    "tags": [],
    "cell_id": "00091-f47e225f-5246-4e5b-b192-000bb83e5219",
    "deepnote_cell_type": "markdown"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00083-94c0db2e-6605-483e-bc6b-c91fa82efcfa",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 14,
    "execution_start": 1622620779100,
    "source_hash": "70d39c6",
    "tags": [],
    "output_cleared": true,
    "deepnote_cell_type": "code"
   },
   "source": "if GENERATE_IAA_SAMPLE == True:\n    with open('../data/annotations/annotation_sample.txt', 'w') as outfile:\n        for tweet in DATA['raw']['hate']['train_text'][-100:]: # last 100 tweets in hate training set\n            outfile.writelines(tweet + '\\n')\n    with open('../data/annotations/ground_truth_annotation_sample.txt', 'w') as outfile:\n        for label in DATA['raw']['hate']['train_labels'][-100:]:\n            outfile.writelines(str(label) + '\\n')",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "Without looking at the gold standard labels from the source of the data, each member of the group individually annotated the 100 tweets and uploaded the file into the folder `data/annotations/manual_annotations`. The following section evaluates the results of this annotation.",
   "metadata": {
    "tags": [],
    "cell_id": "00076-603f59db-4850-4afa-833d-d06e1c2eaa3e",
    "deepnote_cell_type": "markdown"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## Evaluation\n---\nThere exist different metrics for evaluating annotations of multiple annotators, which we will compute within this section. For easy dealing, we read in the manual annotations and load them into a single dataframe, containing both the original tweet, the gold standard label and the four annotated labels by each group member.",
   "metadata": {
    "tags": [],
    "cell_id": "00093-d2583796-19e9-43c4-887a-1e565459a12b",
    "deepnote_cell_type": "markdown"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00085-0c6885fd-8014-4ce7-bfbd-bf43e305bc0a",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 67,
    "execution_start": 1622620779114,
    "source_hash": "76f9350c",
    "tags": [],
    "output_cleared": true,
    "deepnote_cell_type": "code"
   },
   "source": "# compare manual annotations through building dataframe\nannotation_evaluation = pd.DataFrame({'tweets': DATA['raw']['hate']['train_text'][-100:], 'ground_truth': DATA['raw']['hate']['train_labels'][-100:]})\n\nfor filename in os.listdir('../data/annotations/manual_annotations'): # opening manually annotated files\n    with open(f'../data/annotations/manual_annotations/{filename}', 'r') as infile:\n        annotation_evaluation[filename[:-4]] = [int(line.strip()) for line in infile.readlines()]",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00086-60a0157a-ff3a-421b-afc1-6ea25b5459e4",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 67,
    "execution_start": 1622620779193,
    "source_hash": "75b8187d",
    "tags": [],
    "output_cleared": true,
    "deepnote_cell_type": "code"
   },
   "source": "annotation_evaluation",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "",
   "metadata": {
    "cell_id": "00096-97844893-ab52-44a3-b4f0-504c9fa4c326",
    "output_cleared": true,
    "deepnote_cell_type": "visualization"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "### Classification Report \n---\nOne way of evaluating the annotation is to treat each annotator as a machine learning model that predicted some label. Then comparing the predicted labels with the set gold-standard allows us to compute common evaluation metrics, such as accuarcy, recall and sensitivity and f-scores, in order to evaluate the quality of the annotations in relation to the assumed gold standard.",
   "metadata": {
    "tags": [],
    "cell_id": "00098-f2a0eb96-33d7-475b-9a79-1323acf49f0f",
    "deepnote_cell_type": "markdown"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "#### Nicola",
   "metadata": {
    "tags": [],
    "cell_id": "00083-e9d8e55e-1a1b-4382-ab8f-4f1cd0866c6d",
    "deepnote_cell_type": "markdown"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "print('#'*23 + ' ' + 'Nicola' + ' ' + '#'*23 + '\\n')    \nprint(pd.DataFrame(confusion_matrix(annotation_evaluation['ground_truth'], annotation_evaluation['nicola_annotations']), columns=['Predicted Non-Hate', 'Predicted Hate'], index=['Non-Hate', 'Hate']))\nprint('\\n')\nprint(classification_report(annotation_evaluation['ground_truth'], annotation_evaluation['nicola_annotations'], target_names=[\"Non-Hate\",\"Hate\"]))",
   "metadata": {
    "tags": [],
    "cell_id": "00083-4d25e078-0d02-4bee-9b12-18997dc029fc",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "bcea21dc",
    "execution_start": 1622635336417,
    "execution_millis": 17,
    "output_cleared": true,
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "#### Hugo",
   "metadata": {
    "tags": [],
    "cell_id": "00085-813a7bbe-d384-4cf4-9238-d0f687fb3c35",
    "deepnote_cell_type": "markdown"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "print('#'*23 + ' ' + 'Hugo' + ' ' + '#'*23 + '\\n')    \nprint(pd.DataFrame(confusion_matrix(annotation_evaluation['ground_truth'], annotation_evaluation['hugo_annotations']), columns=['Predicted Non-Hate', 'Predicted Hate'], index=['Non-Hate', 'Hate']))\nprint('\\n')\nprint(classification_report(annotation_evaluation['ground_truth'], annotation_evaluation['hugo_annotations'], target_names=[\"Non-Hate\",\"Hate\"]))",
   "metadata": {
    "tags": [],
    "cell_id": "00086-ef62264d-da40-4f93-8501-3b48a383c3af",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "50399f0",
    "execution_start": 1622635445553,
    "execution_millis": 26,
    "output_cleared": true,
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "#### Mika",
   "metadata": {
    "tags": [],
    "cell_id": "00087-8d24c2ad-703e-4aac-85e2-e9b989611a9b",
    "deepnote_cell_type": "markdown"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "print('#'*23 + ' ' + 'Mika' + ' ' + '#'*23 + '\\n')    \nprint(pd.DataFrame(confusion_matrix(annotation_evaluation['ground_truth'], annotation_evaluation['mika_annotations']), columns=['Predicted Non-Hate', 'Predicted Hate'], index=['Non-Hate', 'Hate']))\nprint('\\n')\nprint(classification_report(annotation_evaluation['ground_truth'], annotation_evaluation['mika_annotations'], target_names=[\"Non-Hate\",\"Hate\"]))",
   "metadata": {
    "tags": [],
    "cell_id": "00088-7d2e3665-56fb-4202-b1db-ffb761c3aac1",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "af8257d",
    "execution_start": 1622635473633,
    "execution_millis": 24,
    "output_cleared": true,
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "#### Aidan",
   "metadata": {
    "tags": [],
    "cell_id": "00089-5d316bcb-e369-47e9-b066-5966d455a8b0",
    "deepnote_cell_type": "markdown"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "print('#'*23 + ' ' + 'Aidan' + ' ' + '#'*23 + '\\n')    \nprint(pd.DataFrame(confusion_matrix(annotation_evaluation['ground_truth'], annotation_evaluation['aidan_annotations']), columns=['Predicted Non-Hate', 'Predicted Hate'], index=['Non-Hate', 'Hate']))\nprint('\\n')\nprint(classification_report(annotation_evaluation['ground_truth'], annotation_evaluation['aidan_annotations'], target_names=[\"Non-Hate\",\"Hate\"]))",
   "metadata": {
    "tags": [],
    "cell_id": "00090-0b4eefc0-45ba-4abc-a33b-60c161143407",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "8b9c4fb5",
    "execution_start": 1622635495874,
    "execution_millis": 11,
    "output_cleared": true,
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "In summary, the confusion matrices and classification reports show that the even in a relatively small sample of only 100 tweets and only four different annotators, hatespeech seems to be perceived differently. Within the group, Nicola was most sensible towards hate-speech, meaning that out of all she labelled the most tweets as being hate speech. This resulted in a high recall score on the hate data and an overall balanced prediction. Hugo, in turn only tagged 15 tweets as being hate speech. His insensitivy towards labelling hatespeech resulted in a high perfect recall score on non-hate and perfect precision on hate, i.e. whenever he labelled something as hatespeech, there is absolutely no doubt that it is actually hatespeech. However, he also falsely predicted a lot of hate-tweets as being non-hate, resulting in a poor recall score on hate tweets. \n\nThe reports show that annotation of language data with regards to the general *state-of-mind* of the author is subject to personal perception. Thus, we have to find ways of agreeing on some gold-standard. The following chapter explores how we can numerically summarise annotations of multiple annotators to agree on target labels.",
   "metadata": {
    "tags": [],
    "cell_id": "00091-7fa4467f-e5fc-46f7-ba68-7b202aebb3ae",
    "deepnote_cell_type": "markdown"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "### Cohen's Kappa\n---\nCohen's Kappa ($\\kappa$) is an evaluation metric that is used to measure inter-rater reliability for labelling (categorical) items. In NLP it is i.e. used to compare the annotations of a single annotator with the original annotations. In general, Cohen's Kappa is preferred over simple percentage of agreement, since it corrects the score by the agreement that would be expected by pure chance. \nCohen's Kappa values range from  from in the interval $\\kappa \\in [-1,1]$, with larger values indicating higher agreement between two classifications. \n\nThe following code cell computes the Cohen's Kappa score for each annotator using the function `cohen_kappa_score` from `sci-kit learn`.",
   "metadata": {
    "tags": [],
    "cell_id": "00100-37543064-497f-4aee-a766-05b09d59042f",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "source": "from sklearn.metrics import cohen_kappa_score\n\n# compute cohen's kappa score for each annotator\nfor person in list(annotation_evaluation)[-4:]:\n    print(f'{person[:-12].title()}:\\t {round(cohen_kappa_score(annotation_evaluation[\"ground_truth\"], annotation_evaluation[person]),2)}')",
   "metadata": {
    "tags": [],
    "cell_id": "00084-4c1bc88c-2ab6-4995-aac4-b91a12fad298",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "23ca6b95",
    "execution_start": 1622636108285,
    "execution_millis": 22,
    "output_cleared": true,
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "The Cohen Kappa scores of the group's annotations suggest a *substantial agreement* with the original labelling, as three out of four of the scores are above 0.61. Hugo's score is less but is still considered *fair*.\n\nOne can summarise, that from the Cohen Kappa scores there is no substantial evidence against the original labelling being invalid.",
   "metadata": {
    "tags": [],
    "cell_id": "00085-3859dad6-3aee-409f-8f10-64929ae41a93",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "markdown",
   "source": "### Fleiss Kappa\n---\nFleiss' Kappa is a statistical measure for assessing the reliability of agreement between a fixed number of annotators when labels to a number of classifying items. In contrast to Cohen's Kappa it is metric between multiple annotators and is not only \n\nThe measure calculates the degree of agreement in classification over that which would be expected by chance.",
   "metadata": {
    "tags": [],
    "cell_id": "00095-bb74ed47-c3eb-4c5f-a5e1-5308d395c15e",
    "deepnote_cell_type": "markdown"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "from statsmodels.stats.inter_rater import fleiss_kappa\n\n# fleiss kappa with ground_truth\nannotation_list = []\nfor row_index in range(len(annotation_evaluation)):\n    zero = len([elm for elm in annotation_evaluation.iloc[row_index, 1:] if elm == 0])\n    annotation_list.append([zero, 5-zero])\n\nannotation_table = np.array(annotation_list)\n\nprint(\"Fleiss's K (Including Ground Truth):\", round(fleiss_kappa(annotation_table),2))\n\n# fleiss kappa without ground_truth\nannotation_list = []\nfor row_index in range(len(annotation_evaluation)):\n    zero = len([elm for elm in annotation_evaluation.iloc[row_index, 2:] if elm == 0])\n    annotation_list.append([zero, 4-zero])\n\nannotation_table = np.array(annotation_list)\n\nprint(\"Fleiss's K (Excluding Ground Truth):\", round(fleiss_kappa(annotation_table),2))",
   "metadata": {
    "tags": [],
    "cell_id": "00100-0bac4d0e-2e41-4be3-acaa-a5b21dc7476e",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "24bffeff",
    "execution_start": 1622637768962,
    "execution_millis": 30,
    "output_cleared": true,
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "In both cases, the Fleiss Kappa score can be interpreted as a *moderate agreement*.\n\nThe fact that the kappa score is higher with the ground truth labels being included tells us that the the 3/4 people who are more in agreement with eachother are more in agreement with the original labelling than Hugo's. ",
   "metadata": {
    "tags": [],
    "cell_id": "00102-9f6504f5-08b3-4913-84f4-414ea05a8ab1",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "markdown",
   "source": "# Classification\n---\n*TASK 4*",
   "metadata": {
    "cell_id": "00013-fc964c9b-2d90-48f7-93b6-89e1c191c669",
    "tags": [],
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "source": "from imblearn.pipeline import Pipeline\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.model_selection import GridSearchCV\n\ndef build_model(dataset):\n    # combining train and validation, because grid search cross validates to find best params\n    X_train = DATA['processed'][dataset]['train_text'] + DATA['processed'][dataset]['val_text']\n    y_train = DATA['processed'][dataset]['train_labels'] + DATA['processed'][dataset]['val_labels']\n\n    X_test, y_test = DATA['processed'][dataset]['test_text'], DATA['processed'][dataset]['test_labels']\n\n    # building pipeline to build classifier\n    # a) vectorise tokenised tweets into sparse matrix \n    # b) term frequency inverse document frequency for better performance\n    # c) support vector machine model\n    classifier = Pipeline([('vect', CountVectorizer(preprocessor=lambda x:x, tokenizer=lambda x:x, stop_words=stopwords.words('english'))),\n                           ('ovs', SMOTE()),\n                           ('tfidf', TfidfTransformer()),\n                           ('clf', SGDClassifier())])\n\n    # hyperparameters to tune\n    parameters = {\n        'vect__max_df': (0.5, 0.7, 0.9),\n        'vect__max_features': (2000, 5000),\n        'vect__stop_words': [None, 'english'],\n        #'tfidf__use_idf': (True, False),\n        #'tfidf__norm': ('l1', 'l2'),\n        'clf__loss': ['hinge', 'log'],\n        #'clf__alpha': (0.00001, 1e-3),\n        #'clf__penalty': ('l2', 'elasticnet'),\n    }\n\n    grid = GridSearchCV(classifier, param_grid = parameters, n_jobs=-1, scoring='accuracy')\n    grid = grid.fit(X_train, y_train) # fit\n\n    print('Found Optimal Parameters!')\n    print(f'Parameters: {grid.best_params_}')\n    print(f'Best Macro F1-Score: {grid.best_score_}')\n\n    return grid",
   "metadata": {
    "tags": [],
    "cell_id": "00099-f9b40fcf-5e09-4120-8f5b-00cab15fa2b8",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "9079e31a",
    "execution_start": 1622651650588,
    "execution_millis": 2,
    "output_cleared": true,
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## Binary Classification\n---",
   "metadata": {
    "tags": [],
    "cell_id": "00100-900128a5-dbf4-4453-b0c0-f940ec3f206c",
    "deepnote_cell_type": "markdown"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "hate_model = build_model(dataset = 'hate')",
   "metadata": {
    "tags": [],
    "cell_id": "00100-05dfbf2c-c788-448e-bac1-a425a720be8c",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "f3f06f31",
    "execution_start": 1622651651539,
    "execution_millis": 324836,
    "output_cleared": true,
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# evaluate\nX_test, y_test = DATA['processed']['hate']['test_text'], DATA['processed']['hate']['test_labels']\n\npredictions = hate_model.predict(X_test)\nprint(classification_report(y_test, predictions))",
   "metadata": {
    "tags": [],
    "cell_id": "00102-b0d91fb4-0aff-4c2a-9e80-fabd580d875a",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "2abb931d",
    "execution_start": 1622651976363,
    "execution_millis": 102,
    "output_cleared": true,
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## Multiclass Classification\n---",
   "metadata": {
    "tags": [],
    "cell_id": "00102-eb0a2ff3-ae30-4b18-b4aa-0712e0743b98",
    "deepnote_cell_type": "markdown"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "emotion_model = build_model(dataset = 'emotion')",
   "metadata": {
    "tags": [],
    "cell_id": "00104-dcd713f2-4b2f-4740-ae6f-5fbe31153559",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "20519055",
    "execution_start": 1622651976459,
    "execution_millis": 90470,
    "output_cleared": true,
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# evaluate\nX_test, y_test = DATA['processed']['emotion']['test_text'], DATA['processed']['emotion']['test_labels']\n\npredictions = emotion_model.predict(X_test)\nprint(classification_report(y_test, predictions))",
   "metadata": {
    "tags": [],
    "cell_id": "00100-1923422e-d33e-4d4e-93db-9177324d89f0",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "cabbfb93",
    "execution_start": 1622652066907,
    "execution_millis": 29,
    "output_cleared": true,
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "######################################################## NICOLA",
   "metadata": {
    "tags": [],
    "cell_id": "00106-b1a4cebc-d067-4693-bdc0-bc32638697fa",
    "output_cleared": true,
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## Helper Functions\n---",
   "metadata": {
    "tags": [],
    "cell_id": "00093-871598a9-d4da-4712-bee2-20fae0b2cc46",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00100-5e6acd7d-4ae7-4e78-9e6a-b66efaf41580",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "dbeec2a5",
    "execution_start": 1622641316310,
    "execution_millis": 1,
    "output_cleared": true,
    "deepnote_cell_type": "code"
   },
   "source": "def remove_stop_uncommon(tokenised, vocab, min_frequency, remove_stop=False):\n    \"\"\"removes stop words and words that occur less than the min frequency in the training data, from tokenised\n    inputs:tokenised: tokenised data, min_frequency: integer indicating min frequency of word to be kept,vocab: the vocab of the training data,remove_stop:True/False\"\"\"\n    \n    # initialising set of stop words\n    stop_words = set()\n    if remove_stop == True:\n        stop_words = set(stopwords.words('english')) #set to downloaded stop word list\n\n    #creating helper dictionary of with key being uniques words and value being the frequency\n    vocab_dict = {vocab['token'][x]:vocab['frequency'][x] for x in range(vocab.shape[0])} \n    \n    #for token_list in tokenised: #if not in the temp vocab dict then append as 0 to avoid key error\n    #    for token in token_list:\n    #        try: vocab_dict[token] = vocab_dict[token]\n    #        except: vocab_dict[token] = 0\n\n    return [[token for token in tweet if token not in stop_words and vocab_dict[token] >= min_frequency] for tweet in tokenised] #list comprehention that returns a tokenised version with the correct words removed(depending on parameters chosen)",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00105-32635f8a-7599-47ab-a279-bbcddabc843a",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "db7f57c9",
    "execution_start": 1622620779603,
    "execution_millis": 19,
    "output_cleared": true,
    "deepnote_cell_type": "code"
   },
   "source": "def dummy(doc):\n    \"\"\"does nothing\"\"\"\n    return doc\n    \ndef vectorise(dataset, tokenised, tvt=\"val\"):\n    \"\"\"formats the data into a format that is readable by the models, a sparce matrix\n    input: dataset should be either \"hate\" or \"emotion\", tokenised should be tokenised data. tvt should be \"train\" if formatting training data \"\"\"\n    \n    # initialising count vectoriser without further parameters and fitting data\n    cv = CountVectorizer() \n    cv.fit(DATA[\"processed\"][dataset][\"train_text\"]) # fit to training text(creates empty sparce matrix)\n\n    if tvt==\"train\": #creates the values in the sparce matrix for the relevant data \n        X_train_counts = cv.transform(DATA[\"processed\"][dataset][\"train_text\"])\n    else:\n        X_train_counts = cv.transform(tokenised)        \n    return X_train_counts #returns sparce matrix",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00138-769fb443-40df-48e1-aca5-422ec5f2acbf",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "7da2669f",
    "execution_start": 1622620779647,
    "execution_millis": 15,
    "output_cleared": true,
    "deepnote_cell_type": "code"
   },
   "source": "ros_ = ROS(random_state=123)\ndef fit_model(Classifier,Dataset,min_frequency=1,balance=True,remove_stop_=True):\n    \"\"\"Creates a model\n    Inputs:Classifier:\"RFC\"/\"MNB\"/\"Baseline\",Dataset:\"emotion\"/\"hate\",min_frequency=1 dont think we will use (could remove parameter),balance and reomve_stop are booleans that determine if data is balanced and if stop words are removed\"\"\"\n    if balance==True:#balancing the data using RandomOverSampler \n        X, y = ros_.fit_resample(text_format (Dataset,\n    remove_stop_uncommon(DATA[\"processed\"][Dataset][\"train_text\"],min_frequency,VOCABULARY[Dataset],remove_stop=remove_stop_),tvt=\"train\"),\n    DATA['processed'][Dataset]['train_labels'])\n    else:# set to unbalanced data\n        X = text_format(Dataset,remove_stop_uncommon(DATA[\"processed\"][Dataset][\"train_text\"],min_frequency,VOCABULARY[Dataset],remove_stop=True),tvt=\"train\")\n        y = DATA['processed'][Dataset]['train_labels']\n    if Classifier==\"RFC\":##build relevant model\n        model = RandomForestClassifier(max_depth=None, n_estimators=100,random_state=0)\n    elif Classifier==\"MNB\":\n        model=MultinomialNB(alpha=0.)### this alpha value at its defaut may actually give better results\n    elif Classifier==\"Baseline\":\n        model = SGDClassifier(loss='log')\n    return model.fit(X,y)#fit the model and return",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00115-99889d7e-eae9-4cf6-92c3-461424a27d14",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "495240d1",
    "execution_start": 1622620779667,
    "execution_millis": 10,
    "output_cleared": true,
    "deepnote_cell_type": "code"
   },
   "source": "def model_accuracy_val(min_frequency,Dataset,model,v_or_t=\"val\",remove_stop_=False):\n    \"\"\"takes cleaned and tokensed data and returns accuracy score of the model\n    input:min_frequency:int(see remove_stop_uncommon),Dataset:\"hate\"/\"emotion\",model:a fitted model,v_or_t:\"test\" if using test data,remove_stop_:boolean value if val/test should remove stop words\"\"\"\n    if v_or_t !=\"val\":#warn and set to \"test\" if v_or_t is changed\n        v_or_t=\"test\"\n        print(\"warning! you are using the test data\")\n    prediction = model.predict(text_format(Dataset,remove_stop_uncommon(DATA[\"processed\"][Dataset][f\"{v_or_t}_text\"],min_frequency,VOCABULARY[Dataset],remove_stop=remove_stop_),tvt=v_or_t))\n    actual = DATA[\"processed\"][Dataset][f\"{v_or_t}_labels\"]\n    return accuracy_score(actual, prediction)#return accuracy score at the given parameters",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00102-9187ee3f-3a66-4ae7-aeec-1bba42ca44f2",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "5e84bbe4",
    "execution_start": 1622624244832,
    "execution_millis": 1,
    "output_cleared": true,
    "deepnote_cell_type": "code"
   },
   "source": "def model_prf_score(Dataset,min_frequency,model,v_or_t=\"val\",remove_stop__=False):\n    \"\"\"takes cleaned and tokensed data and returns precision, recall and F-score of the model\n    inputs:Dataset:\"hate\"/\"emotion\",min_frequency:int(see remove_stop_uncommon),model:a fitted model,v_or_t:\"test\" if using test data,remove_stop_:boolean value if val/test should remove stop words\"\"\"\n    if v_or_t !=\"val\":#warn and set to \"test\" if v_or_t is changed\n        v_or_t=\"test\"\n        print(\"warning! you are using the test data\")\n    prediction = model.predict(text_format(Dataset,remove_stop_uncommon(DATA[\"processed\"][Dataset][f\"{v_or_t}_text\"],min_frequency,VOCABULARY[Dataset],remove_stop=remove_stop__),tvt=v_or_t))\n    actual = DATA[\"processed\"][Dataset][f\"{v_or_t}_labels\"] \n    scores= precision_recall_fscore_support(actual,prediction)#save scores into scores\n    order=list(DATA[\"raw\"][Dataset][\"mapping\"].values())# this gives the labels for the colums and rows of the dataframe\n    score_labels=[\"Precision\",\"Recall\",\"F-score\", \"Support\"]\n    dataframe_of_scores=pd.DataFrame(data=scores,columns=order,index=score_labels)\n    list_of_accuracy=[model_accuracy_val(min_frequency,Dataset,model,v_or_t=\"val\",remove_stop_=remove_stop__) for x in range(len(order))]\n    dataframe_of_accuracy=pd.DataFrame(data=np.array([list_of_accuracy]), columns=order, index=[\"Accuracy\"])\n    #print(dataframe_of_accuracy)\n\n    return pd.concat([dataframe_of_scores,dataframe_of_accuracy])#return pandas df with scores",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00118-73a12a7e-da36-45e8-b192-e97c0a1bf8d5",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "3b31293",
    "execution_start": 1622620779699,
    "execution_millis": 10,
    "output_cleared": true,
    "deepnote_cell_type": "code"
   },
   "source": "def plot_accuracy(max_vocab,Dataset,model,ax,remove_stop__=False):\n    \"\"\"plots onto a subplot: the accuracy score of 1 model given validation data with different min_freq values\n    input:max_vocab:int shows accuracy in range [0,max_vocab],Dataset:\"hate\"/\"emotion\",model:a fitted model,ax:an axis for the sublot e.g.[0,0],remove_stop__:boolean value if val should remove stop words\"\"\"\n    accuracies=[]\n    for i in range (max_vocab):# for each min_freq=i, find accuracy score and append to accuracies\n        accuracies.append(model_accuracy_val(i,Dataset,model,remove_stop_=remove_stop__))\n    ax.xaxis.set_major_locator(MaxNLocator(integer=True))#plot integer values on the xaxis\n    ax.plot([x for x in range (max_vocab)],accuracies)\n    ax.set_xlabel(\"Minimum Word Frequecy\")\n    ax.set_ylabel(\"Accuracy Score\")\n    ax.set_title(\"Accuracy and Minimum Word Frequecy in Validation Data\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00118-0538e68a-1280-45ba-82c6-6c7ea200259e",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "50b9a70",
    "execution_start": 1622633260940,
    "execution_millis": 1,
    "output_cleared": true,
    "deepnote_cell_type": "code"
   },
   "source": "def plot_prf_accuracy(Dataset,max_freq,model_list,model_names,v_or_t_=\"val\"):\n    scores=[\"Precision\",\"Recall\",\"F-score\",\"Accuracy\"]#list of scores here\n    if v_or_t_ !=\"val\":#warn and set to \"test\" if v_or_t is changed\n        v_or_t_=\"test\"\n        print(\"warning! you are using the test data\")\n    num_labels=len(list(DATA[\"raw\"][Dataset][\"mapping\"].values()))\n    fig,ax=plt.subplots(num_labels,4,figsize=(20,3*num_labels))\n    fig.tight_layout(pad=0.4, w_pad=1, h_pad=3.0)\n    fig.suptitle(f\"PRF and Accuracy Scores for {Dataset} {model_names[-1]} Models\",y=1.1,fontsize=\"x-large\")\n    for x,model in enumerate(model_list):#each column\n        dataframes=[]\n        for i in range(max_freq):\n            dataframes.append(model_prf_score(Dataset,i,model,v_or_t=v_or_t_))\n        for y,label in enumerate(list(DATA[\"raw\"][Dataset][\"mapping\"].values())):#each row\n            ax[y,x].set_title(f\"{label.title()} and {model_names[x]}\") \n            ax[y,x].set_ylim(0,1)\n            ax[y,x].xaxis.set_major_locator(MaxNLocator(integer=True))#plot integer values on the xaxis\n            for score in scores:#each line\n                y_values=[]\n                for i in range(max_freq):\n                    value=dataframes[i][label][score]\n                    y_values.append(value)\n                ax[y,x].plot([x for x in range (max_freq)],y_values,label=score)\n    handles, labels = ax[y,x].get_legend_handles_labels()\n    fig.legend(handles, labels, loc='upper right',ncol=3,fontsize=\"small\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Loading in Models",
   "metadata": {
    "tags": [],
    "cell_id": "00095-b929a20c-4bf0-410d-b0a3-3eeda945705f",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "markdown",
   "source": "#### Binary",
   "metadata": {
    "tags": [],
    "cell_id": "00121-1fe7cf7a-0d2d-4009-ad1f-1af1a7f419a1",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00117-1d894bd8-9abd-4e26-a05e-1ab2497dc08b",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "b5515dda",
    "execution_start": 1622626095503,
    "execution_millis": 2846,
    "output_cleared": true,
    "deepnote_cell_type": "code"
   },
   "source": "Baseline_hate=fit_model(\"Baseline\",\"hate\",balance=False,remove_stop_=False)\nBaseline_hate_bal=fit_model(\"Baseline\",\"hate\",remove_stop_=False)\nBaseline_hate_stop=fit_model(\"Baseline\",\"hate\",balance=False)\nBaseline_hate_bal_stop=fit_model(\"Baseline\",\"hate\" )",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00104-7f7659a1-79c0-4738-85d1-9e08b112947e",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "33ef6cce",
    "execution_start": 1622623774504,
    "execution_millis": 2147,
    "output_cleared": true,
    "deepnote_cell_type": "code"
   },
   "source": "MNB_hate=fit_model(\"MNB\",\"hate\",balance=False,remove_stop_=False)\nMNB_hate_bal=fit_model(\"MNB\",\"hate\",remove_stop_=False)\nMNB_hate_stop=fit_model(\"MNB\",\"hate\",balance=False)\nMNB_hate_bal_stop=fit_model(\"MNB\",\"hate\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00128-20a84ca6-312f-48ee-ad26-7c1abb5a27a3",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "572c09f",
    "execution_start": 1622623881777,
    "execution_millis": 41581,
    "output_cleared": true,
    "deepnote_cell_type": "code"
   },
   "source": "RFC_hate=fit_model(\"RFC\",\"hate\",balance=False,remove_stop_=False)\nRFC_hate_bal=fit_model(\"RFC\",\"hate\",remove_stop_=False)\nRFC_hate_stop=fit_model(\"RFC\",\"hate\",balance=False)\nRFC_hate_bal_stop=fit_model(\"RFC\",\"hate\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "#### Multiclass",
   "metadata": {
    "tags": [],
    "cell_id": "00125-fd2f8bcd-93bf-42bf-8b7a-cd0d504ec231",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00113-57e54bbf-c758-4d21-8a2c-ff56f887d149",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "b62120d9",
    "execution_start": 1622623798495,
    "execution_millis": 952,
    "output_cleared": true,
    "deepnote_cell_type": "code"
   },
   "source": "Baseline_emotion=fit_model(\"Baseline\",\"emotion\",balance=False,remove_stop_=False)\nBaseline_emotion_bal=fit_model(\"Baseline\",\"emotion\",remove_stop_=False)\nBaseline_emotion_stop=fit_model(\"Baseline\",\"emotion\",balance=False)\nBaseline_emotion_bal_stop=fit_model(\"Baseline\",\"emotion\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00110-2981b2e8-246d-46f9-ad77-e990deb6af2a",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "7869b19c",
    "execution_start": 1622623786588,
    "execution_millis": 801,
    "output_cleared": true,
    "deepnote_cell_type": "code"
   },
   "source": "MNB_emotion=fit_model(\"MNB\",\"emotion\",balance=False,remove_stop_=False)\nMNB_emotion_bal=fit_model(\"MNB\",\"emotion\",remove_stop_=False)\nMNB_emotion_stop=fit_model(\"MNB\",\"emotion\",balance=False)\nMNB_emotion_bal_stop=fit_model(\"MNB\",\"emotion\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00132-dcbf3895-f1b5-4d73-b904-c8cceb3ef955",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "8a6f8a95",
    "execution_start": 1622623857715,
    "execution_millis": 15333,
    "output_cleared": true,
    "deepnote_cell_type": "code"
   },
   "source": "RFC_emotion=fit_model(\"RFC\",\"emotion\",balance=False,remove_stop_=False)\nRFC_emotion_bal=fit_model(\"RFC\",\"emotion\",remove_stop_=False)\nRFC_emotion_stop=fit_model(\"RFC\",\"emotion\",balance=False)\nRFC_emotion_bal_stop=fit_model(\"RFC\",\"emotion\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Running Models on Validation Data",
   "metadata": {
    "tags": [],
    "cell_id": "00135-7d7129b1-1ad3-4955-a510-e9abc4277853",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "markdown",
   "source": "#### Binary\nEvaluation of performance:<br>\n(If F-score is interpreted as most important) <br>\nBest Baseline: unclear, however the models trained on unbalanced data should not be used. <br>\nBest MNB: unclear, however the models trained on unbalanced data should not be used.  <br>\nBest RFC: models trained on balanced data are better.<br>\n<br>_\nBest Overall: Random Forrest, balanced, stop words irrelevant so remove_stop=False, no imporvement with small increase min freq so min_freq=1 should be used(to be expected with this classifier) <br>",
   "metadata": {
    "tags": [],
    "cell_id": "00130-00581053-c4bd-4a88-9688-99478a5b7a6e",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00124-14e750e2-6163-4796-a50c-7460eacd54cb",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "3d4c111e",
    "execution_start": 1622628637609,
    "execution_millis": 224528,
    "output_cleared": true,
    "deepnote_cell_type": "code"
   },
   "source": "models=[Baseline_hate,Baseline_hate_bal,Baseline_hate_stop,Baseline_hate_bal_stop]\nmodel_names=[\"Baseline_hate\",\"Baseline_hate_bal\",\"Baseline_hate_stop\",\"Baseline_hate_bal_stop\",\"Baseline SGD Classifier\"]\nplot_prf_accuracy(\"hate\",50,models,model_names)",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00124-4c3d99ca-bdca-4ae6-8725-0da4fdb485f4",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "60442e33",
    "execution_start": 1622628881002,
    "execution_millis": 224291,
    "output_cleared": true,
    "deepnote_cell_type": "code"
   },
   "source": "models=[MNB_hate,MNB_hate_bal,MNB_hate_stop,MNB_hate_bal_stop]\nmodel_names=[\"MNB_hate\",\"MNB_hate_bal\",\"MNB_hate_stop\",\"MNB_hate_bal_stop\",\"MNB Classifier\"]\nplot_prf_accuracy(\"hate\",50,models,model_names)",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00123-5df00fa6-88eb-48c5-8491-073a128971ac",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "18706b3f",
    "execution_start": 1622629105322,
    "execution_millis": 262929,
    "output_cleared": true,
    "deepnote_cell_type": "code"
   },
   "source": "models=[RFC_hate,RFC_hate_bal,RFC_hate_stop,RFC_hate_bal_stop]\nmodel_names=[\"RFC_hate\",\"RFC_hate_bal\",\"RFC_hate_stop\",\"RFC_hate_bal_stop\",\"Random Forest Classifier\"]\nplot_prf_accuracy(\"hate\",50,models,model_names)",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "#### Mulitclass\nEvaluation:<br>\n (If F-score is interpreted as most important) <br>\nBest Baseline: <br>\nBest MNB: <br>\nBest RFC: Balanced with our without stop words<br>",
   "metadata": {
    "tags": [],
    "cell_id": "00134-51a43ddb-b77e-4bb1-aa2f-618b6dd43ee3",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00120-f24d7d72-5732-46ba-8f15-72f46fc28472",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "1cf651e",
    "execution_start": 1622629567917,
    "execution_millis": 162648,
    "output_cleared": true,
    "deepnote_cell_type": "code"
   },
   "source": "models=[Baseline_emotion,Baseline_emotion_bal,Baseline_emotion_stop,Baseline_emotion_bal_stop]\nmodel_names=[\"Baseline_emotion\",\"Baseline_emotion_bal\",\"Baseline_emotion_stop\",\"Baseline_emotion_bal_stop\",\"Baseline SGD Classifier\"]\nplot_prf_accuracy(\"emotion\",50,models,model_names)",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00119-84635f1e-f233-4c3c-afed-2220a1a5098c",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "4351bdbe",
    "execution_start": 1622629872277,
    "execution_millis": 162633,
    "output_cleared": true,
    "deepnote_cell_type": "code"
   },
   "source": "models=[MNB_emotion,MNB_emotion_bal,MNB_emotion_stop,MNB_emotion_bal_stop]\nmodel_names=[\"MNB_emotion\",\"MNB_emotion_bal\",\"MNB_emotion_stop\",\"MNB_emotion_bal_stop\",\"MNB Classifier\"]\nplot_prf_accuracy(\"emotion\",50,models,model_names)",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00119-813dc6bd-4e25-4cf8-b14b-54bdbb7b7a08",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "7a8f2466",
    "execution_start": 1622630057642,
    "execution_millis": 194902,
    "output_cleared": true,
    "deepnote_cell_type": "code"
   },
   "source": "models=[RFC_emotion,RFC_emotion_bal,RFC_emotion_stop,RFC_emotion_bal_stop]\nmodel_names=[\"RFC_emotion\",\"RFC_emotion_bal\",\"RFC_emotion_stop\",\"RFC_emotion_bal_stop\",\"Random Forest Classifier\"]\nplot_prf_accuracy(\"emotion\",50,models,model_names)",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "#### Results With Test Data",
   "metadata": {
    "tags": [],
    "cell_id": "00138-da4c0e3d-ed0e-4a15-8b83-6546516af7b3",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "source": "model_prf_score(\"hate\",1,M_hate_bal)###best scoring validation",
   "metadata": {
    "tags": [],
    "cell_id": "00120-0061d974-5436-4b8a-8090-dee1ff67d858",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "c276267",
    "execution_start": 1622632495669,
    "execution_millis": 1296,
    "output_cleared": true,
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "model_prf_score(\"hate\",5,MNB_hate_bal,v_or_t=\"test\")###test data",
   "metadata": {
    "tags": [],
    "cell_id": "00121-ac3e83a6-6185-48f3-8017-77437bf51026",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "3292c0b1",
    "execution_start": 1622633444130,
    "execution_millis": 1276,
    "output_cleared": true,
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "models=[RFC_hate,RFC_hate_bal,RFC_hate_stop,RFC_hate_bal_stop]\nmodel_names=[\"RFC_hate\",\"RFC_hate_bal\",\"RFC_hate_stop\",\"RFC_hate_bal_stop\",\"Random Forest Classifier\"]\nplot_prf_accuracy(\"hate\",10,models,model_names,v_or_t_=\"test\")",
   "metadata": {
    "tags": [],
    "cell_id": "00123-c582e010-9f2e-415a-b00f-1f617db687ff",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "fd2cb041",
    "execution_start": 1622633298963,
    "execution_millis": 59498,
    "output_cleared": true,
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "confusion_matrix(actual,prediction, labels=None, sample_weight=None, normalize=None)",
   "metadata": {
    "tags": [],
    "cell_id": "00149-2a2b129b-347e-4aa3-a7ef-16b7d02c4590",
    "output_cleared": true,
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## To Do:\n* task 2 descriptive things\n* paper \n* make the formatting of the notebook good enough to be readable\n* understanding laplace or better estimate instead -smoothing\n* improvements to model (4):\n- [x] remove min freq\n- [x] random forrest\n- [x] stopwords (make comparable)\n- [x] Multinomial NB model -why and whats different \n* make some graphics to compare values other than accuracy\n\nNici:\n* plot_prf_accuracy comments\n* summary of graphs\n* running with test data\n* graphics\n\n",
   "metadata": {
    "tags": [],
    "cell_id": "00111-1da73e0b-ac3f-41e9-a123-902545ab4b24",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "markdown",
   "source": "important note: if we add a smooting on the multinomialNB by removing the alpha=0. parmeter then the results are better but different. worth looking into as a group, can ammend functions later to accomodate- nicola",
   "metadata": {
    "tags": [],
    "cell_id": "00141-be9f2143-a19b-4249-a372-73b067980637",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "markdown",
   "source": "",
   "metadata": {
    "tags": [],
    "cell_id": "00149-8ebc4d62-8511-478e-9001-155bdd421769",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "markdown",
   "source": "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=7ea65742-831a-48cd-9d56-a22e1ed66c7b' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>",
   "metadata": {
    "tags": [],
    "created_in_deepnote_cell": true,
    "deepnote_cell_type": "markdown"
   }
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "deepnote": {
   "is_reactive": false
  },
  "deepnote_execution_queue": [],
  "deepnote_notebook_id": "912c8bda-48f1-43bf-8bff-0aac9dd9fbcb",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 }
}