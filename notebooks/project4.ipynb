{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00000-8f8eb98f-2a98-41af-9825-5a8a8c441dee",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "# Project 4: XXX\n",
    "---\n",
    "\n",
    "**Group 9: Aidan Stocks, Hugo Reinicke, Nicola Clark, Jonas-Mika Senghaas**\n",
    "\n",
    "Submission: *03.06.2021* / Last Modified: *27.04.2021*\n",
    "\n",
    "---\n",
    "\n",
    "This notebook contains the step-by-step data science process performed *XXX*. The goal of this project was to *XXX*.\n",
    "\n",
    "The initial data was obtained from the [TweetEval](https://github.com/cardiffnlp/tweeteval#evaluating-your-system) GitHub repository, that provides data for supervised training of classifiers for natural language processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00001-5fcfa044-05ac-485a-8293-0db98813ff5e",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "## Introduction\n",
    "---\n",
    "*XXX*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00002-e3f8c8e5-1dc7-47ff-9108-2450997c051c",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "## Running this Notebook\n",
    "---\n",
    "This notebook contains all code to reproduce the findings of the project as can be seen on the [GitHub](https://github.com/jonas-mika/fyp2021p04g09) page of this project. In order to read in the data correctly, the global paths configured in the section `Constants` need to be correct. The following file structure - as prepared in the `submission.zip` - was followed throughout the project and is recommended to use (alternatively the paths in the section `Constants` can be adjusted):\n",
    "\n",
    "```\n",
    "*project tree structure*\n",
    "```\n",
    "*Note that the rest of the file structure as can be seen on the [GitHub](https://github.com/jonas-mika/fyp2021p03g09) page of the project generates automatically*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00003-8f195f8a-4025-446a-8b44-300b55666211",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "## Required Libraries and Further Imports\n",
    "---\n",
    "Throughout the project, we will use a range of both built-in and external Python libraries. This notebook will only run if all libraries and modules are correctly installed on your local machines. \n",
    "To install missing packages use `pip install <package_name>` (PIP (Python Package Index) is the central package management system, read more [here](https://pypi.org/project/pip/)). \n",
    "\n",
    "In case you desire further information about the used packages, click the following links to find detailed documentations:\n",
    "- [Pandas](https://pandas.pydata.org/)\n",
    "- [Numpy](https://numpy.org/)\n",
    "- [Matplotlib](https://matplotlib.org/stable/index.html)\n",
    "- [PIL](https://pillow.readthedocs.io/en/stable/)\n",
    "- [SciKit Learn](https://scikit-learn.org/stable/)\n",
    "- [SciKit Image](https://scikit-image.org/)\n",
    "- [Scipy](https://www.scipy.org/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00004-cf13b571-fc4d-40f4-8617-43224164fcb8",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 9285,
    "execution_start": 1619685254742,
    "source_hash": "64948f79",
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "# uncomment lines with uninstalled packages\n",
    "\n",
    "#!pip install -U numpy pandas matplotlib seaborn skikit-learn \n",
    "#!pip install pycontractions\n",
    "#!pip install python-Levenshtein #/root/venv/lib/python3.7/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.warnings.warn(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00005-9cbe0305-7f30-486b-be81-0c10af861246",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 0,
    "execution_start": 1619685862727,
    "source_hash": "fac8fee5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# python standard libraries\n",
    "import json                                            # data transfer to and from json format\n",
    "import os                                              # access operating system from python\n",
    "import random                                          # creates randomness\n",
    "import re                                              # regex search in python\n",
    "\n",
    "# external libraries\n",
    "import numpy as np                                     # used for numerical calculations and fast array manipulations\n",
    "import pandas as pd                                    # provides major datastructure pd.DataFrame() to store datasets\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt                        # basic data visualisation\n",
    "import seaborn as sns                                  # advanced data visualisation\n",
    "from nltk.tokenize import TweetTokenizer               # tokeniser api\n",
    "from pycontractions import Contractions                # Intelligently expands and creates contractions in text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00006-5afa390e-a19c-4f7b-b2f5-752fba00777d",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 57,
    "execution_start": 1619685398251,
    "source_hash": "cf01d3e0"
   },
   "outputs": [],
   "source": [
    "print(f'Numpy Version: {np.__version__}')\n",
    "print(f'Pandas Version: {pd.__version__}')\n",
    "print(f'Matplotlib Version: {matplotlib.__version__}')\n",
    "print(f'Seaborn Version: {sns.__version__}')\n",
    "#print(f'Pycontractions Version: {Contractions.__version__}') # wont work dont know why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00006-e9706598-54b2-4f29-9ca9-a6660685b48c",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "Since this project makes heavy use of functions to achieve maximal efficiency, all functions are stored externally in the package structure `project3'. The following imports are necessary for this notebook to run properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00007-14d719d0-f91a-4eac-adc0-8f3981566bf1",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 2,
    "execution_start": 1619683192963,
    "source_hash": "905c667e",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#from project4.processing import ...\n",
    "#from project4.save import ...\n",
    "#from project4.features import ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00008-4f347db9-f433-4a57-9e83-4fbb27586617",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "## Constants\n",
    "---\n",
    "To enhance readibilty, as well as to decrease the maintenance effort, it is useful for bigger projects to define contants that need to be accessed globally throughout the whole notebook in advance. \n",
    "The following cell contains all of those global constants. By convention, we write them in caps (https://www.python.org/dev/peps/pep-0008/#constants)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00009-8c033e56-02ab-48b7-9eff-1efefe08d35c",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 4,
    "execution_start": 1619683192970,
    "source_hash": "b9d56086",
    "tags": []
   },
   "outputs": [],
   "source": [
    "DATASETS = ['hate', 'emotion']\n",
    "\n",
    "# store paths\n",
    "PATH = {}\n",
    "PATH['data'] = {}\n",
    "PATH['data']['raw'] = \"../data/raw/\"\n",
    "PATH['data']['processed'] = \"../data/processed/\"\n",
    "\n",
    "# store data \n",
    "DATA = {}\n",
    "for dataset in DATASETS:\n",
    "    DATA[dataset] = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00010-30f29e03-2c93-4a41-9cdd-c63ddcf81385",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "*TASK 0*\n",
    "# Fetching Data\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00012-517a92e6-641e-4b58-ae16-4fc3f1dc0f0c",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "## Loading in Data\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00011-d60e995b-fcb6-4868-b5f3-bead2a4d235a",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 4,
    "execution_start": 1619683192981,
    "source_hash": "fc69abe6",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def read_raw_input(dataset):\n",
    "    # reading in all .txts into list of strings\n",
    "    for _file in os.listdir(f'../data/raw/{dataset}'):\n",
    "        with open(f'../data/raw/{dataset}/{_file}', 'r', encoding='UTF-8') as infile:\n",
    "            DATA[dataset][_file[:-4]] = [line.strip() for line in infile.readlines()]\n",
    "\n",
    "    # convert target labels to integers\n",
    "    for key in ['train_labels', 'val_labels', 'test_labels']:\n",
    "        DATA[dataset][key] = [int(x) for x in DATA[dataset][key]]\n",
    "\n",
    "    # convert mapping to dictionary\n",
    "    DATA[dataset]['mapping'] = {int(string.split('\\t')[0]): string.split('\\t')[1] for string in DATA[dataset]['mapping']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00014-a564b5fd-9112-40ea-ab20-a9e06cef672c",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 47,
    "execution_start": 1619683192988,
    "scrolled": false,
    "source_hash": "3fbf30af"
   },
   "outputs": [],
   "source": [
    "for dataset in DATASETS:\n",
    "    read_raw_input(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00015-6ac2c66d-4d76-4711-82ed-29c0f25e3314",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "## Exploring Data\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00016-1ade1bb5-34a5-4a97-9049-49dbb00ee154",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "### Peek into Training Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00017-8c456ad3-3f69-4ec0-931f-2fdc6fe0e33e",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 44,
    "execution_start": 1619683193038,
    "source_hash": "8ad7720"
   },
   "outputs": [],
   "source": [
    "# hate\n",
    "for i in range(10):\n",
    "    print(f\"{i+1}\\tLabel: {DATA['hate']['mapping'][DATA['hate']['train_labels'][i]].title()}\\t\\t{DATA['hate']['train_text'][i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00018-f6587a74-f6da-4c6c-8540-db447f5615d9",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 17,
    "execution_start": 1619683193066,
    "source_hash": "9d51bce4"
   },
   "outputs": [],
   "source": [
    "# hate\n",
    "for i in range(10):\n",
    "    print(f\"{i+1}\\tLabel: {DATA['emotion']['mapping'][DATA['emotion']['train_labels'][i]].title()}\\t\\t{DATA['emotion']['train_text'][i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00019-ac0093ee-cb8e-4df0-9240-28b8639da8f0",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "### Visualising Label Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00020-ff2b603d-7d95-43b5-8411-7c54ff2985ec",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 0,
    "execution_start": 1619683193067,
    "source_hash": "61ecae75"
   },
   "outputs": [],
   "source": [
    "def visualise_label_distribution(dataset):\n",
    "    fig, ax = plt.subplots(ncols=3, figsize=(12,4))\n",
    "    fig.suptitle(f'Frequency of Target Label in {dataset.capitalize()}', fontsize=12, fontweight='bold')\n",
    "\n",
    "    for i, key in enumerate(['train_labels', 'val_labels', 'test_labels']):\n",
    "        label, count = np.unique(DATA[dataset][key], return_counts=True)\n",
    "        ax[i].bar(label, count, color='grey');\n",
    "        ax[i].set_title(key.replace('_', ' ').title())\n",
    "        ax[i].set_xticks(label); ax[i].set_xticklabels([string.title() for string in DATA[dataset]['mapping'].values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00021-046d0488-77dc-47d3-a29c-d178ea704a1f",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 294,
    "execution_start": 1619683193068,
    "source_hash": "894be1c8"
   },
   "outputs": [],
   "source": [
    "visualise_label_distribution(dataset='hate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00022-83c0ec2d-8094-4123-8603-caad77edb2af",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 483,
    "execution_start": 1619683193359,
    "source_hash": "c0926180"
   },
   "outputs": [],
   "source": [
    "visualise_label_distribution(dataset='emotion')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00011-c1d3e256-83ab-4833-93bb-155c6dcb73ca",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "*TASK 0.5*\n",
    "# Processing of Language Data\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00024-925deca3-bb20-43eb-bc18-98e47d7a5780",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "### Tokenize Tweets\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00024-20c29398-21ac-493b-8d2a-2161b831ef91",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 0,
    "execution_start": 1619683193857,
    "source_hash": "a1f7603b"
   },
   "outputs": [],
   "source": [
    "def tokeniser1(tweet):\n",
    "    return re.findall('\\w+', tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00013-de37fee0-8080-49a2-8348-0cc7cd793435",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 0,
    "execution_start": 1619683193858,
    "source_hash": "fcce8e84",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tokeniser2(tweet):\n",
    "    return re.split(' ', tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00026-12aa65c5-fb65-4563-9518-2593493305f8",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 0,
    "execution_start": 1619683193858,
    "source_hash": "addc91b"
   },
   "outputs": [],
   "source": [
    "def tokeniser3(tweet):\n",
    "    tk = TweetTokenizer()\n",
    "    return tk.tokenize(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00027-ef6625a3-be50-47af-8b54-b6da6068f0c4",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 6,
    "execution_start": 1619683193859,
    "source_hash": "70f25dd3"
   },
   "outputs": [],
   "source": [
    "def tokeniser4(line):\n",
    "    # Initialise lists\n",
    "    tokens = []\n",
    "    unmatchable = [] # should be emtpy if done nicely\n",
    "\n",
    "    # Compile patterns for speedup\n",
    "    token_pat = re.compile('\\w+')\n",
    "    skippable_pat = re.compile('\\s+')  # typically spaces\n",
    "\n",
    "    # As long as there's any material left...\n",
    "    while line:\n",
    "        print(line)\n",
    "        # Try finding a skippable token delimiter first.\n",
    "        skippable_match = re.search(skippable_pat, line)\n",
    "        print(skippable_match)\n",
    "        if skippable_match and skippable_match.start() == 0:\n",
    "            # If there is one at the beginning of the line, just skip it.\n",
    "            line = line[skippable_match.end():]\n",
    "        else:\n",
    "            # Else try finding a real token.\n",
    "            token_match = re.search(token_pat, line)\n",
    "            if token_match and token_match.start() == 0:\n",
    "                print(token_match)\n",
    "                # If there is one at the beginning of the line, tokenise it.\n",
    "                tokens.append(line[:token_match.end()])\n",
    "                line = line[token_match.end():]\n",
    "            else:\n",
    "                # Else there is unmatchable material here.\n",
    "                # It ends where a skippable or token match starts, or at the end of the line.\n",
    "                unmatchable_end = len(line)\n",
    "                if skippable_match:\n",
    "                    unmatchable_end = skippable_match.start()\n",
    "                if token_match:\n",
    "                    unmatchable_end = min(unmatchable_end, token_match.start())\n",
    "                # Add it to unmatchable and discard from line.\n",
    "                unmatchable.append(line[:unmatchable_end])\n",
    "                line = line[unmatchable_end:]\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00028-15e617bc-1809-4253-8f95-c827822c2e9e",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 19,
    "execution_start": 1619683193867,
    "source_hash": "d98db666"
   },
   "outputs": [],
   "source": [
    "#ex_tweet = DATA['hate']['train_text'][0]\n",
    "ex_tweet = \"https://t.co/9z2J3P33Uc FB needs to hurry up and add a laugh/cry button ðŸ˜¬ðŸ˜­ðŸ˜“ðŸ¤¢ðŸ™„ðŸ˜± Since eating my feelings has not fixed the world's problems, I guess I'll try to sleep... HOLY CRAP: DeVos questionnaire appears to include passages from uncited sources https://t.co/FNRoOlfw9s well played, Senator Murray Keep the pressure on: https://t.co/4hfOsmdk0l @datageneral thx Mr Taussig It's interesting how many people contact me about applying for a PhD and don't @user spell my name #something right. Wow ok I didn't realise i'm using contractions like haven't and won't.\"\n",
    "\n",
    "tokeniser = [tokeniser1, tokeniser2, tokeniser3]\n",
    "\n",
    "print('Original Tweet: ',ex_tweet, '\\n')\n",
    "for i in range(len(tokeniser)):\n",
    "    print(f\"Tokeniser {i+1}: {tokeniser[i](ex_tweet)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00029-6b025e1e-41c8-40e8-95a6-ce98b58a469d",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "### Summary\n",
    "---\n",
    "\n",
    "Tokeniser 1 (Finds all Words):\n",
    "+ Good representation of words (includes Unicode characters and is sensible to punctuation within words, ie. \"don't\")\n",
    "- Deletes all punctuation (which might be important, ie. twitter communication heavily based tagging using '@' and hastags (#))\n",
    "- Destroys Links\n",
    "- Blind to Emojis\n",
    "\n",
    "Tokeniser 2 (Split at Whitespace):\n",
    "+ Overall good representation of words \n",
    "+ keeps punctuation\n",
    "- ending punctuation is not being separated\n",
    "+ keeps Links\n",
    "+ recognises Emojis\n",
    "- doesnt separate adjacent emojis \n",
    "- blind to several words written together (in slang), ie. idontgiveafuck\n",
    "\n",
    "Tokeniser 3 (Twitter Tokeniser API, Overcomes issues of Tokeniser 2):\n",
    "+ Ending punctuation is being separated\n",
    "+ Separate adjacent Emojis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00032-9918e5ff-ad34-4397-8a23-02f60d16a194",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "### Improving Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00032-4555a69b-e985-4c97-8a63-e922bdc51ee5",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "Improving on the Tweet Tokenizer:\n",
    "* lowercasing \n",
    "* expanding contractions\n",
    "* change links to a dummy value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00033-ca3b7101-abf1-48c1-ac45-4c16b8f2cb52",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "Functions: (most likely to combine later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00033-c821a417-c72a-4e30-b57d-d26f3af2b5f2",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 2,
    "execution_start": 1619684911748,
    "source_hash": "9378b39e",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def lowercase(tokenised_list):\n",
    "    return [i.lower() for i in tokenised_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00035-574798eb-0296-46e6-8efa-66e796637c32",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 3,
    "execution_start": 1619684961292,
    "source_hash": "1f018063",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def rm_links(tokenised_list):\n",
    "    dummy= \"thisisalinkadp2yNQ263fh7fRhGvKw\" #randomised password generator used to get a most likely unique string to include\n",
    "    return [token if token[:8]!=\"https://\" else dummy for token in tokenised_list] #check here later that all links are in fact in this format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# or specify any model from the gensim.downloader api\n",
    "cont = Contractions(api_key=\"glove-twitter-100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00037-33cee70e-a038-49f8-b073-2acbda49bedf",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 1,
    "execution_start": 1619686304718,
    "source_hash": "663e33b5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# testing out\n",
    "list(cont.expand_texts([\"I'd like to know how I'd done that!\",\n",
    "                        \"We're going to the zoo and I don't think I'll be home for dinner.\",\n",
    "                        \"Theyre going to the zoo and she'll be home for dinner.\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00036-7e43d308-5788-4976-9e1b-74ae65eba3d5",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 1,
    "execution_start": 1619689077652,
    "source_hash": "e734c3c9",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def expand(tweet): # may be worth doing first on whole text before tokeniseing\n",
    "    return list(cont.expand_texts([tweet]))[0]\n",
    "\n",
    "print(expand(ex_tweet))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00035-c25b2302-1132-486c-aa17-17ca52315596",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "Testing:\n",
    "\n",
    "Without contractions expansion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00036-0d890806-572c-47ed-a4a7-bec4bf93f950",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 5,
    "execution_start": 1619689629483,
    "source_hash": "67f47c7a",
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(ex_tweet, \"\\n\")\n",
    "ex_tokenised=tokeniser3(ex_tweet) # tokenised before contraction expansion\n",
    "print(ex_tokenised, \"\\n\")\n",
    "print(lowercase(ex_tokenised), \"\\n\")\n",
    "print(rm_links(ex_tokenised), \"\\n\") # check later "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(ex_tweet, \"\\n\")\n",
    "print(expand(ex_tweet), \"\\n\")\n",
    "ex_tokenised = tokeniser3(expand(ex_tweet)) # tokenised after contractions expansion\n",
    "print(ex_tokenised, \"\\n\")\n",
    "print(lowercase(ex_tokenised), \"\\n\")\n",
    "print(rm_links(ex_tokenised), \"\\n\") # check later "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00041-a87844dc-3c23-44f3-ba3b-9e706f69a073",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "Suggested order:\n",
    "* lowercase\n",
    "* contractions # build in later locally or do a simple sometimes wrong version ourselves\n",
    "* tokenize \n",
    "* remove links\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00042-494c027a-c333-45d2-95c0-f8775d64c2ee",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "## Final Tokenising"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00043-2f55d04c-3946-4a00-a740-a8debdffb56c",
    "deepnote_cell_type": "code",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def rm_links(tokenised_list):\n",
    "    \"\"\"removes links from tokenised tweets\"\"\"\n",
    "    # was thinking isntead of using a password generator we make dummy something like \"-=#LINK#=-\" because i doubt anyone will have written that and it's easier to recognise\n",
    "    dummy = \"-=#LINK#=-\"\n",
    "    #dummy= \"thisisalinkadp2yNQ263fh7fRhGvKw\" #randomised password generator used to get a most likely unique string to include\n",
    "    return [token if token[:8]!=\"https://\" else dummy for token in tokenised_list] #check here later that all links are in fact in this format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00043-6453b1f6-fe2d-4454-a660-25658b074aab",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 1,
    "execution_start": 1619689337156,
    "source_hash": "8dfd8804",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def expand(tweet): # build in the ml model for contraciton here later\n",
    "    return list(cont.expand_texts([tweet]))[0]\n",
    "\n",
    "    # may be easier to contract everything contractable? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00042-a9e30c04-e319-4b86-93a2-495d377ffdf7",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 0,
    "execution_start": 1619689488735,
    "source_hash": "e151d5cf",
    "tags": []
   },
   "outputs": [],
   "source": [
    "tk = TweetTokenizer()\n",
    "def clean(tweet):\n",
    "    lower=tweet.lower()\n",
    "    contractions=expand(lower)# build in the ml model for contractions here\n",
    "    tokenised=tk.tokenize(contractions)\n",
    "    link_dummy=rm_links(tokenised)\n",
    "    return link_dummy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing clean function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00046-0a1e72d3-a195-4b4d-b338-b99415bcfd4a",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 13,
    "execution_start": 1619689533083,
    "scrolled": true,
    "source_hash": "53c4d941",
    "tags": []
   },
   "outputs": [],
   "source": [
    "clean(ex_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00046-8f2c5b62-9cce-4460-a430-78de3377ebca",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 0,
    "execution_start": 1619690997427,
    "source_hash": "4625d5cf",
    "tags": []
   },
   "outputs": [],
   "source": [
    "for dataset in [\"emotion\",\"hate\"]:\n",
    "    for key in ['train_text', 'val_text', 'test_text']:\n",
    "        # create file path if not there already\n",
    "        # if there already then emmpty file\n",
    "        for i in key:\n",
    "            pass\n",
    "            # write each clean(DATA[dataset][key]) to the relevant document in data tokenised with respective path out of the 6 possible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00012-f7ed8c9e-4e5c-4499-8cbd-bc465deeb219",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "*TASK 2*\n",
    "# XXX\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00015-c0d67151-ed60-49d4-822d-0806e71831bc",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 3,
    "execution_start": 1619683193875,
    "source_hash": "5cd17fc",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# some code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00013-fc964c9b-2d90-48f7-93b6-89e1c191c669",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "*TASK 3*\n",
    "# Open Question: XXX\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00017-2bcdfccd-1c4e-4b00-abcb-2807c0f7da9c",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 1,
    "execution_start": 1619683193881,
    "source_hash": "5cd17fc",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# some code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "created_in_deepnote_cell": true,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=7ea65742-831a-48cd-9d56-a22e1ed66c7b' target=\"_blank\">\n",
    "<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\n",
    "Created in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>"
   ]
  }
 ],
 "metadata": {
  "deepnote": {
   "is_reactive": false
  },
  "deepnote_execution_queue": [],
  "deepnote_notebook_id": "912c8bda-48f1-43bf-8bff-0aac9dd9fbcb",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
